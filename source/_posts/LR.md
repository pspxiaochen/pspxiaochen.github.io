---
categories:
  - 机器学习
mathjax: true
copyright: true
date: 2018-08-23 16:01:23
title: 逻辑回归推导与整理
description: 哈哈哈哈
---
逻辑回归虽然名字里带着‘回归’，但是他其实是一种分类的方法，可以用于二分类问题。根据线性回归我们知道，需要先找到一个预测函数$h(x)$,显然我们知道，该函数的输出必须是两个值（分别代表2个类别），所以利用了Logistic函数（或者称为Sigmoid函数），函数形式为：$$g(z)=\frac{1}{1+e^-z}$$
对应的函数图像是一个取值在0到1之间的S。
##那么我们为什么要用sigmoid函数呢。
1.首先我们先看看sigmoid自身的好处。
a.sigmoid函数连续，单调递增
b.关于（0.0.5）中心对称
c.求导非常容易，所以速度很快
d.可以把值变成（0,1）之间，可以表示概率。
2.我们在从指数族来考虑
指数族分布的形式为：$$p(y;η)=b(y)exp(η^TT(y)−α(η))$$
伯努利分布，高斯分布，泊松分布，贝塔分布，狄特里特分布都属于指数分布。
在逻辑回归时，我们认为函数概率服从伯努利分布，伯努利分布的概率可以表示成
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![此处输入图片的描述][1]

  $$η = log (φ/(1-φ))$$
  $$φ=\frac{1}{1+e^{-η}}$$
可以看到，η的形式的logistic函数一致，这是因为logistic模型对问题的前置概率估计是伯努利分布的缘故。这时$η=θx$（具体可以看一下统计学习方法78页中logistic函数）。


我们整理一下写成$$h_θ(x)=\frac{1}{1+e^{-θx}}$$
其中x为样本的输入，$h_θ(x)$为模型输出，可以理解为某一分类的概率大小。而θ为分类模型的要求出的模型参数。对于模型输出hθ(x)，我们让它和我们的二元样本输出y（假设为0和1）有这样的对应关系，如果hθ(x)>0.5 ，即xθ>0, 则y为1。如果hθ(x)<0.5，即xθ<0, 则y为0。
hθ(x)的值越小，而分类为0的的概率越高，反之，值越大的话分类为1的的概率越高。如果靠近临界点，则分类准确率会下降。
理解了二元分类回归的模型，接着我们就要看模型的损失函数了，我们的目标是极小化损失函数来得到对应的模型系数θ。
##构造损失函数
我们第一先想到还是用线性回归的平方损失函数，但是我们把h(x)带入损失函数后会发现是一个非凸函数，这就意味着代价函数有着许多的局部最小值，这不利于我们的求解。 

我们现在换一种思路，我们前面提到h(z(x))可以看做x样本是1类别的概率（z=θx），所以我们有$$p(y=1|x;\theta)=h_θ(x)$$
$$p(y=0|x;\theta)=1-h(wx)=1-h_θ(x)$$
其中$p(y=1|x;\theta)$表示给定w，那么x样本是正类的概率大小。
上面两个式子可以合并一下写成：$$P(y|x;w) = h_θ(x)^y(1-h_θ(x))^{1-y}$$
接下来我们要用极大似然估计来估计参数w。
极大似然估计的思想是这样的：
1.极大似然估计中采样产生的样本需要满足一个重要假设，所有采样的样本都是独立同分布的。 
2.极大似然估计是在模型已定，参数未知的情况下，估计模型中的具体参数。 
3.极大似然估计的核心是让产生所采样的样本出现的概率最大。即利用已知的样本结果信息，反推具有最大可能导致这些样本结果出现的模型的参数值。 
举个例子：
1.假设当前的样本为{(x1,y1=1),(x2,y2=0),(x3,y3=1),(x4,y4=0),(x5,y5=0)}，样本是满足独立同分布的。
2.由于样本是满足独立同分布的，那么出现以上样本分布的总概率为下式，需要让产生这一组样本的概率最大。
$$P=P(Y=1|x=x1)P(Y=0|x=x2)P(Y=1|x=x3)P(Y=0|x=x4)P(Y=0|x=x5)$$
设$ P(Y=1|x)=π(x),P(Y=0|x)=1−π(x)$,则上式可以化为：$$P=\prod_{1}^5[π(x^i)]y^i[1−π(x^i)](1−y^i)$$

所以逻辑回归的似然函数为：$$L(w)=\prod_{1}^Nh_θ(x^i)^{y^i}(1-h_θ(x^i))^{1-y^i}$$
其中N为N个样本点。
由于对一个函数取对数之后单调性不变，不影响结果，所以我们对上面的式子取对数，也叫对数似然函数:
$$L(\theta)=\sum_{1}^N(y^{(i)}ln(h_{\theta}(x^{(i)}))+ (1-y^{(i)})ln(1-h_{\theta}(x^{(i)})))$$
对L（θ）求极大值，可以变相变成对-L(θ)求极小值，再用梯度下降就可以求解。这里就不推了，我在笔记本上推。


##如果用一句话概括逻辑回归就是：逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

记一篇博客 [逻辑回归的常见面试点总结][2]


  [1]: http://wx2.sinaimg.cn/mw690/72fdc620ly1fujr7u5w1wj20ay03cglr.jpg
  [2]: http://www.cnblogs.com/ModifyRong/p/7739955.html