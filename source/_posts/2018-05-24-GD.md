---
title: 我自己理解的梯度下降的原理
date: 2018-05-23 21:27:00
categories:
  - 机器学习
tags:
  - 梯度下降
mathjax: true
description: 思考了一晚上总算是稍微理解一些了！！！开心！
---

&emsp;&emsp;最近不太顺啊，各种碰壁，看来学习确实容不得半点虚假，会就是会，不会就是不会。现在决定再认认真真复习一遍机器学习和数据结构，再来巩固一下自己的知识体系，只希望这次可以别忘的太快。

# 我们为什么要使用梯度下降。
&emsp;&emsp;因为机器学习总的来说是一个优化问题。我们有一个想要优化的函数，比如说损失函数，我们总是希望可以让损失函数变得越来越小。那怎么才能使损失函数变得原来越小呢？损失函数中有一些未知的参数，我们只要不停更新这个参数，就可以让损失函数越来越小。直到损失函数等于0或者不再发生变化为止。所以说梯度下降是用来更新一些未知参数的，而这些参数会使损失函数越来越小。



# 梯度下降是怎么来的呢？
&emsp;&emsp;首先我们来看一元函数的泰勒展开，以便于更好的理解多元函数的泰勒展开。如果一个一元函数n阶可导，它的泰勒展开公式为：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![泰勒公式][1]
&emsp;&emsp;如果在某一点处的导数值大于0（+），则函数在此处是增函数，加大x的值函数会增加，减小x的值函数会减小。相反的，如果在某一点处导数值小于0（-），则函数是减函数，增加x的值函数值会减小（+），减小x的值函数会增加。因此我们可以得出一个结论：**如果x的变化很小，并且变化值与导数值反号，则函数值下降。**对于一元函数，x的变化只有两个方向，要么朝左，要么朝右。

&emsp;&emsp;下面我们把这一结论推广到多元函数的情况。多元函数f(x)在x点处的泰勒展开为：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![此处输入图片的描述][2]
&emsp;&emsp;这里我们忽略了二次以及更高的项。其中一次项是梯度向量▽f(x)与自变量的增量△X的內积，这等价于一元函数的一次项f'(X0)(X-X0)。这样，函数的增量与自变量的增量△X、函数梯度的关系可以表示为：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![此处输入图片的描述][3]
&emsp;&emsp;如果△X足够小，在X的某一领域内，则我们可以忽略二次及以上的项，有：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![此处输入图片的描述][4]
&emsp;&emsp;这里的情况比较复杂，△X是一个向量，有无穷多种方向，该往哪个方向走呢？如果能保证上面约等式的右边恒小于0，则有：**f(x+△x) < f(x)**，即函数值递减，这就是下山的正确方向。如果这句话还不能理解，我们可以再换一种方式理解。假如x是我们想要更新的参数,f(x)是我们刚开始需要优化的损失函数,我们很希望可以让f(x)越来越小，那怎么才能够让其越来越小呢。就如我一开始说的那样我们要不停的更新里面的参数,而x就是我们要更新的那个参数。用数学表达式就是**min(f(x+△x)）**，如果**f(x+△x) < f(x)**恒成立，那就是说，我们的优化函数一直再减小，那么可以理解为一定会找到一个完美的结果，是不是想想就有点小开心呢，但是往往不会有那么好的事情的，就和人生一样。
&emsp;&emsp;再接上面，那我们怎么样才能保证约等式的右边恒小于0呢，因为有：
&emsp;&emsp;&emsp;&emsp;![此处输入图片的描述][5]
&emsp;&emsp;在这里，||·||表示向量的模，θ是向量▽f(x)和△x的夹角。因为向量的模一定大于等于0，所以说如果cosθ <= 0,则能保证约等式的右边恒小于0。也就是选择合适的增量△x，就能保证函数数值下降，要达到这种效果，只要保证梯度和△x的夹角的余弦值小于等于0就行。由于cosθ >= -1，当θ = π时，cosθ有极小值-1，此时梯度和△x反向，夹角为180度，因此当向量△X的模大小一定时，当：△X = - ▽f(x),也就是在梯度相反的方向函数值下降最快。此时有cosθ = -1。**重点来了：那我们为什么要让△X = - ▽f(x)呢，让它等于别的不可以吗？这一块我考虑了好半天，最后我只能这么解释一下。如果我们想让约等式的右边小于等于0恒成立，怎么办！！怎么样才能保证最靠谱呢？那我不如让△X = - ▽f(x)算了，一个平方的值带负号是一定小于等于0的。这也太靠谱了吧！！我觉得这么取得意义就在这里，仅此而已。**
&emsp;&emsp;也就是说只要梯度不为0，往梯度的反方向走函数值一定是下降的。直接用△X = - ▽f(x)可能会有问题，因为x+△x可能会超出x的领域范围之外，此时是不能忽略泰勒展开中的二次及以上的项的，因此步伐不能太大，一般设△x = -α▽f(x)。其中α为一个接近于0的正数，称为步长，由人工设定。
&emsp;&emsp;从初始点X_0开始，使用此迭代公式：X_k+1 = X_k - α▽f(x).
&emsp;&emsp;只要没有到达梯度为0的点，则函数值会沿着序列X_k递减，最终会收敛到梯度为0的点，这就是梯度下降法。迭代终止的条件是函数的梯度值为0（实际是接近于0），此时认为已经达到了极值点。注意我们找到的是梯度为0的点，这不一定就是极值点。梯度下降法只需要计算函数在某些点处的梯度，实现简单，计算量小。
&emsp;&emsp;再写一套比较直观的公式推导，看完有点原来如此，so easy的感觉~~
&emsp;&emsp;$f(x_{k+1}) = f(x_k + x_{k+1} - x_k) = f(x_k) + ▽f(x_k)(x_{k+1} - x_k)$
&emsp;&emsp;$▽f(x_k)$即为函数f在$x_k$处的梯度。
&emsp;&emsp;为了使$f(x_{k+1}) < f(x_k)$,则需要$▽f(x_k)(x_{k+1} - x_k)<0$
&emsp;&emsp;则只需要使$x_{k+1}=x_k - \gamma▽f(x_k)$即可。
&emsp;&emsp;上式便是梯度下降的迭代公式，其中$\gamma$为学习速率。这样看是不是很清楚了呢。。。


  [1]: http://wx1.sinaimg.cn/mw690/72fdc620ly1frm73g6o3yj20x602uaac.jpg
  [2]: http://wx3.sinaimg.cn/mw690/72fdc620ly1frm7e3gn0zj20mk028t8s.jpg
  [3]: http://wx4.sinaimg.cn/mw690/72fdc620ly1frm7r65pc0j20mu02iq31.jpg
  [4]: http://wx1.sinaimg.cn/mw690/72fdc620ly1frm7taxkn3j20mi02ejrf.jpg
  [5]: http://wx3.sinaimg.cn/mw690/72fdc620ly1frm8akufs2j20mw02adfv.jpg

