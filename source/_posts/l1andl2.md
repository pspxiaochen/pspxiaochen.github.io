---
categories:
  - 机器学习
mathjax: true
copyright: true
date: 2018-09-01 14:28:44
title: 聊一聊L1和L2正则化
description: 下一章聊正则化
---
#正则化
一般来说，监督学习可以看做是最小化下面的目标函数。

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![此处输入图片的描述][1]
其中，第一项L(yi,f(xi;w)) 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但是正如上面所说，我们不仅要保证训练误差最小，我们更希望我们的模型在测试数据中误差小，所以我们要加上第二项，也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。其中这个正则化函数就是我们常见的L0,L1,L2范数。
##正则化的目的：防止过拟合！
##正则化的本质：约束要优化的参数。
&emsp;&emsp;其实如果学了机器学习的算法之后，就会发现，机器学习的大部分带参数的模型都和上面的公式很像，大部分无非就是变换这两项而已。对于第一项损失函数，如果是平方损失函数那就是线性回归，如果是合页损失函数，那就是SVM，如果是对数损失函数，就是逻辑斯特回归。如果是指数损失函数那就是boosting了。不同的LOSS函数具有不同的拟合性质我们得具体问题具体分析，我们先来分析一波规则项Ω(w)。
&emsp;&emsp;规则化函数Ω(w)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同，但我们在论文中常见的都聚集在：零范数、一范数、二范数。
我们先分别来介绍一下L0和L1范数。
#L0范数和L1范数
&emsp;&emsp;  L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。这太直观了，太露骨了吧，换句话说，让参数W是稀疏的。但是我们知道一般想让参数稀疏我们都是通过L1范数来实现的。这就是这节的题目把L0和L1放在一起的原因，因为他们有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？
&emsp;&emsp;L1范数指的是向量中各个元素绝对值只和，也叫‘稀疏规则算子’。为什么L1范数会使权值稀疏？可以回答它是L0范数的最优凸近似，实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。
&emsp;&emsp;上面还有一个问题：既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。
**来个一句话总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。**
那么我们再来总结一下稀疏的好处：
##特征选择
&emsp;&emsp; 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。
##可解释性
&emsp;&emsp; 另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。
#L2范数
&emsp;&emsp; 除了L1范数，还有一种更受宠幸的规则化范数是L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。这用的很多吧，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。
&emsp;&emsp; L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。
**这里也一句话总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。**
#几个问题
##L1范数和L2范数都可以防止过拟合吗？
答：都可以防止过拟合的，加入正则化项就是为了防止过拟合。

##为什么L1范数可以做特征选择（部分参数变成0）？为什么L2范数可以让参数变的小
**我们先从数学的角度分析：**
&emsp;&emsp; &emsp;&emsp; &emsp;&emsp; ![此处输入图片的描述][2]
我们来看一下，两个式子不一样的地方只有被框框住的地方。L1的话会减少sign(w_i)倍的$\eta\frac{\lambda}{n}$，而L2的话会减少wi倍的$\eta\frac{\lambda}{n}$,当w_i在[1,正无穷）时候，L2的减小速率比L1的更快，当w_i在（0,1）的时候，L1的比L2获得更快的减小速率，当w越来越小时，L1更容易接近0，而L2更不容易变化。因此L1会得到更多的0。
还有一种比较有意思的数学解释可以看一下知乎这个链接：

[l1 相比于 l2 为什么容易获得稀疏解？][3]

**从概率的角度分析：**
![此处输入图片的描述][4]

**从优化角度分析**
![此处输入图片的描述][5]

可以把之前的问题换成带条件的优化问题。

![此处输入图片的描述][6]
高维我们无法想象，简化到2维的情形，如上图所示。其中，左边是L1图示，右边是L2图示，左边的方形线上是L1中w1/w2取值区间，右边得圆形线上是L2中w1/w2的取值区间，绿色的圆圈表示w1/w2取不同值时平方误差项的值的等高线（凸函数），从等高线和w1/w2取值区间的交点可以看到，L1中两个权值倾向于一个较大另一个为0，L2中两个权值倾向于均为非零的较小数。这也就是L1稀疏，L2平滑的效果。

##为什么说参数越小模型就越简单。
模型过于复杂是因为模型尝试去兼顾各个测试数据点，导致模型函数处于一种动荡的状态， 每个点的到时在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。而参数变小了之后会让导数的绝对值也变小，这样的话曲线就会变的比之前平滑。


  [1]: http://wx3.sinaimg.cn/mw690/72fdc620ly1fuu1fqswbvj20am01q748.jpg
  [2]: http://wx2.sinaimg.cn/mw690/72fdc620ly1fuu3x8dmbrj20x408lq3s.jpg
  [3]: https://www.zhihu.com/question/37096933
  [4]: http://wx4.sinaimg.cn/mw690/72fdc620ly1fuu65qc4iej210e0nzacs.jpg
  [5]: http://wx2.sinaimg.cn/mw690/72fdc620ly1fuu6ffng2aj20z303ldg0.jpg
  [6]: http://wx1.sinaimg.cn/mw690/72fdc620ly1fuu6hy7lmvj20w80hgk4a.jpg