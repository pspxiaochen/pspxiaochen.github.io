{"meta":{"title":"发条晨","subtitle":"愿我走出半生，归来仍是少年","description":null,"author":"pspxiaochen","url":"https://pspxiaochen.club"},"pages":[{"title":"","date":"2018-06-27T02:12:33.202Z","updated":"2018-06-27T02:12:33.055Z","comments":true,"path":"baidu_verify_8q2qaNXys5.html","permalink":"https://pspxiaochen.club/baidu_verify_8q2qaNXys5.html","excerpt":"","text":"8q2qaNXys5","raw":null,"content":null},{"title":"about","date":"2017-08-18T15:28:22.000Z","updated":"2017-11-20T07:39:44.771Z","comments":true,"path":"about/index.html","permalink":"https://pspxiaochen.club/about/index.html","excerpt":"","text":"自我介绍 &emsp;&emsp;&ensp;我是pspxiaochen,也可以叫我发条晨，目前是一名计算机专业在读研究生，喜欢编程，喜欢各类主机，喜欢任天堂，喜欢sony，喜欢电影，追剧。最近也开始喜欢一些文化类节目，锵锵三人行，圆桌派。然后呢，希望通过写博客记录下学习的点点滴滴，记录下自己的成长之路……","raw":null,"content":null},{"title":"categories","date":"2017-08-18T15:28:08.000Z","updated":"2017-11-20T07:24:06.654Z","comments":true,"path":"categories/index.html","permalink":"https://pspxiaochen.club/categories/index.html","excerpt":"","text":"","raw":null,"content":null},{"title":"tags","date":"2017-08-18T15:24:46.000Z","updated":"2017-11-20T07:24:06.656Z","comments":true,"path":"tags/index.html","permalink":"https://pspxiaochen.club/tags/index.html","excerpt":"","text":"","raw":null,"content":null}],"posts":[{"title":"线性回归与最小二乘法","slug":"costfunction","date":"2018-07-03T04:27:00.000Z","updated":"2018-07-05T09:33:12.695Z","comments":true,"path":"costfunction/","link":"","permalink":"https://pspxiaochen.club/costfunction/","excerpt":"","text":"&emsp;&emsp;我们假设正确的结果y和我们的预测的输出函数有如下关系： &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$y^{(i)} = \\theta^Tx^{(i)} + \\epsilon^{(i)}$ 在这里$\\theta^Tx^{(i)}$为我们的预测函数，$\\epsilon^{(i)}$是和真实值的误差。因为每个样本都是独立的，因此误差直接也是独立的。所以我们假设$\\epsilon^{(i)}$服从期望是0（我们希望没有误差）,方差是$\\sigma^2$的高斯分布，记作$\\epsilon^{(i)}\\sim N(0,\\sigma^2)$,而高斯分布的概率密度函数为： &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$&emsp;&emsp;将误差带入上面的式子得：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P(\\epsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2})$ 因为我们假设$\\epsilon^{(i)}$服从期望是0（我们希望没有误差）,方差是$\\sigma^2$的高斯分布。所以还可以假设$y^{(i)}$是服从期望是里$\\theta^Tx^{(i)}$，方差为$\\sigma^2$的高斯分布，在给定 x(i)且参数为 θ的情况下，记作：$y^{(i)}\\sim N(\\theta^Tx^{(i)},\\sigma^2)$,所以我们可以将上面的式子改写为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$ 因为在我们的样本中，y(i) 已经给定了，我们需要找到一个参数 θ，使得我们最有可能去得到 y(i)的分布。我们想要估计其中的未知参数θ。由此我们可以想到一个非常常用的参数估计方法—极大似然轨迹。关于极大似然估计我推荐知乎的2片文章，讲的浅显易懂。似然函数与极大似然估计一文搞懂极大似然估计 接着刚才，我们使用极大似然估计后可写成：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L(\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta) $ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L(\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$因为是极大似然估计，所以我们希望$L(\\theta)$要尽可能的大。所以我们对上面的式子取对数，因为对数不改变函数的单调性。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L(\\theta)=\\sum_{i=1}^m log\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$=mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}+\\sum_{i=1}^m -\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$=mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\sum_{i=1}^m \\frac{1}{2\\sigma^2}*(y^{(i)}-\\theta^Tx^{(i)})^2$ 为了使$L(\\theta)$要尽可能的大，我们就需要让$J(\\theta)=\\frac{1}{2}*(y^{(i)}-\\theta^Tx^{(i)})^2$尽量的小，所以就有了平方损失函数，可以看到是一模一样的。J(θ) 即为此线性回归的cost function。由此我们可以非常自然地推导出为什么线性回归中的cost function是使用最小二乘法。接下来就是求解过程，常用的就是梯度下降，如果想知道为什么用梯度下降，请看这篇我自己理解的梯度下降原理","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"线性回归","slug":"线性回归","permalink":"https://pspxiaochen.club/tags/线性回归/"}]},{"title":"25周岁生日","slug":"birthday","date":"2018-06-29T11:27:00.000Z","updated":"2018-06-30T04:31:13.138Z","comments":true,"path":"birthday/","link":"","permalink":"https://pspxiaochen.club/birthday/","excerpt":"","text":"今天是25岁生日，祝我生日快乐，下午接了一个团购鞋子，真是讽刺。","raw":null,"content":null,"categories":[{"name":"life","slug":"life","permalink":"https://pspxiaochen.club/categories/life/"}],"tags":[]},{"title":"机器学习之交叉验证","slug":"cross-validation","date":"2018-06-29T02:27:00.000Z","updated":"2018-06-29T03:32:36.146Z","comments":true,"path":"cross-validation/","link":"","permalink":"https://pspxiaochen.club/cross-validation/","excerpt":"","text":"交叉验证的基本思想&emsp;&emsp; 交叉验证的基本想法是重复的使用数据；把给定数据进行切分，将切分的数据集组合为训练集与测试集，再次基础上反复地进行训练、测试、以及模型选择。 简单的交叉验证1、首先随机得将已给数据分为两部分，一部分作为训练集，一部分作为测试集。（一般是73分）2、然后用训练集在各种条件下（比如不同的参数）训练模型，从而得到不同的模型；3、在测试集上评价各个模型的测试误差，选出测试误差最小的模型。优点：由于测试集和训练集是分开的，就避免了过拟合现象 k折交叉验证1.首先将训练数据平均切分成k份，每一份互不相交且大小一样。2.用k-1个子集进行训练模型，用余下的那一个作为预测。3.将2这一过程对可能的k种选择重复进行。4.最后选出k次测评中测试误差最小的。、优点：这个方法充分利用了所有样本。但计算比较繁琐，需要训练k次，测试k次。 留一法留一法就是每次只留下一个样本做测试集，其它样本做训练集，如果有k个样本，则需要训练k次，测试k次。优点：留一发计算最繁琐，但样本利用率最高。适合于小样本的情况。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"交叉验证","slug":"交叉验证","permalink":"https://pspxiaochen.club/tags/交叉验证/"}]},{"title":"求余和取模傻傻分不清","slug":"rem&mod","date":"2018-06-27T13:27:00.000Z","updated":"2018-06-27T12:21:38.761Z","comments":true,"path":"rem&mod/","link":"","permalink":"https://pspxiaochen.club/rem&mod/","excerpt":"","text":"&emsp;&emsp;通常情况下取模运算(mod)和求余(rem)运算被混为一谈，因为在大多数的编程语言里，都用’%’符号表示取模或者求余运算。在这里要提醒大家要十分注意当前环境下’%’运算符的具体意义，因为在有负数存在的情况下，两者的结果是不一样的。&emsp;&emsp;假设有整数a和b，取模或者求余运算的方法都是（1）c=a/b (2)r=a-c*b&emsp;&emsp;求模运算和求余运算在第一步不同,取余运算在计算商值向0方向舍弃小数位,取模运算在计算商值向负无穷方向舍弃小数位.&emsp;&emsp;在C中 %是取余，mod是取模。&emsp;&emsp;在Python中%就是取模。 例子：如果a=3 b=-43/(-4)等于-0.75在取余运算时候商值向0方向舍弃小数位为0在取模运算时商值向负无穷方向舍弃小数位为-1所以3rem(-4) = 33mod(-4) = -1希望这次可以记住。","raw":null,"content":null,"categories":[{"name":"笔试","slug":"笔试","permalink":"https://pspxiaochen.club/categories/笔试/"}],"tags":[{"name":"mod","slug":"mod","permalink":"https://pspxiaochen.club/tags/mod/"}]},{"title":"kd树-第一次面试的痛","slug":"kd-tree","date":"2018-06-26T13:27:00.000Z","updated":"2018-06-27T02:54:08.510Z","comments":true,"path":"kd-tree/","link":"","permalink":"https://pspxiaochen.club/kd-tree/","excerpt":"","text":"什么是KD树&emsp;&emsp;kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分。构造kd树相当于不断的用垂直于坐标轴的超平面将K维空间切分，构成一系列的k维超矩形区域。kd树的每个节点对应于一个K维超矩形区域。 构建KD树输入：K维空间数据集，样本数为N。$x_i = (x_i^{(1)},x_i^{(2)},x_i^{(3)}….x_i^{(k)})^T$输出：KD树&emsp;&emsp;(1)开始：构造根节点，根节点包含了K维空间的超矩形区域。&emsp;&emsp;选择$x^{(1)}$作为第一个切分的维度，找到$x^{(1)}$坐标的中位数为切分点，将数据集一分为二，大于$x_i^{(1)}$的中位数在右，小于在左。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。&emsp;&emsp;将落在切分超平面的实例点保存在根节点。&emsp;&emsp;(2)重复切分操作，再将$x^{(2)}$作为切分的维度，并在之前已经一分为二的左边和右边分别找到$x^{(2)}$维度的中位数，接着分别切分到左右两边。&emsp;&emsp;(3)一直重复切分操作,对深度为j的节点,选择$x^{(l)}$为切分的坐标轴，l=j(mod)k +1,以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超平面矩形区域切分为两个子区域。将落在切分超平面的实例点保存在根节点。&emsp;&emsp;(4)直到两个子区域没有实例存在时停止。从而形成kd数的区域划分。 搜索kd树（最近邻）&emsp;&emsp;下面介绍如何利用kd树进行最近邻搜索。利用kd树可以省去大部分数据点的搜索，从而减少搜索的计算量。输入：（1）已经构造好的kd树（2）目标点x（想找到距离x最近的点）输出： x的最近邻&emsp;&emsp;（1）在kd树种找出包含目标点x的叶节点：从根节点出发，递归的向下访问kd树。若目标点x当前维（用哪个维度分的就用哪个维度）的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点。直到找到子节点为叶节点为止。&emsp;&emsp;（2）以次叶节点为‘当前最近点’&emsp;&emsp;（3）递归（刚才一路下来的那些点都要进行接来下来的操作）地向上回退，在每个结点都进行以下操作：&emsp;&emsp;&emsp;&emsp;a:如果该结点保存的实例点比当前最近点的距离（用选定的公式计算比如欧氏距离）目标点更近，则以该实例点为‘当前最近点’。&emsp;&emsp;&emsp;&emsp;b:检查目前结点的另一子结点对应的区域是否以目标点为球心、以目标点与‘当前最近点’间的距离为半径的超球体相交。&emsp;&emsp;&emsp;&emsp;若相交：可能在另一个子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点，接着递归的向下搜索&emsp;&emsp;&emsp;&emsp;如果不想交：向上回退。&emsp;&emsp;（4）当退回到根节点时，搜索结束。最后的‘当前最近点’即为x的最近邻点。 &emsp;&emsp;kd数搜索的评论计算复杂度是O(logN),N是训练实例数.kd树更适用于训练实例数远大于空间维数时的k近邻搜索。 搜索kd树*（K近邻）输入：给定一个已经构建好的kd树，需要寻找的点P，需要寻找几个近邻K。输出：L列表，L列表里有K个空位，每个空位装的是近邻点。 &emsp;&emsp;（1）根据切分的维度对比同维度下P的坐标值向下搜索。（也就是说，如果树的节点是照 $x_r=a$ 进行切分，并且 p 的 r 坐标小于 a，则向左枝进行搜索；反之则走右枝）。&emsp;&emsp;（2）当到达叶子结点时，将其标记为访问过。如果L列表中不足k个点，则将当前结点的坐标加入到L列表。如果L列表不为空并且当前结点与P的距离小于L中所存放结点的最大距离，则用当前结点替换掉L中离P最远的结点。&emsp;&emsp;（3）如果当前的结点不是整棵树最顶端的结点，执行a操作；反之，输出L列表。&emsp;&emsp;&emsp;&emsp;a:向上爬一个结点。如果当前（向上爬过之后的）结点没有被访问过，将其标记为访问过，然后执行b和c操作；如果当前结点被访问过，再次执行a.&emsp;&emsp;&emsp;&emsp;b:如果此时L列表中不足k个点，则将结点加入到L；如果L中已经满k个点，并且当前结点与P的距离小于 L列表中最远的距离的点，则用结点替换掉L中距离P最远的点。&emsp;&emsp;&emsp;&emsp;c:计算p和当前结点切分线的距离（作垂线）。如果该距离大于等于L列表中距离最远的点并且L列表中已经有k个点，则在当前结点的另外一边（没访问过的那边）不会有更近的点，接着执行（3）；如果该距离小于L列表中距离最远的点或者L列表中不足k个点，则切分线另一边可能有更近的点，因此在当前结点的另外一边可能有更近的点，因此在当前结点的另外一边从（1）开始重新执行。 大概过成就是这样，下面有一个链接，链接里有距离寻找K近邻的例子。搜索kd数k近邻搜索样例","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"kd树","slug":"kd树","permalink":"https://pspxiaochen.club/tags/kd树/"}]},{"title":"该死的牛客网输入。","slug":"code_input","date":"2018-06-25T13:27:00.000Z","updated":"2018-06-26T15:57:20.237Z","comments":true,"path":"code_input/","link":"","permalink":"https://pspxiaochen.club/code_input/","excerpt":"","text":"Python各种输入的方法。例一&emsp;&emsp;输入：1247 15 9 5 可以看到 第一行的输入的数是第二行输入数字的个数。1234import sysline = sys.stdin.readline()n = int(line) #读进来的都是str类型，需要转换成int类型nums = [int(t) for t in sys.stdin.readline().split()] 在这里我们主要看一下第4行代码12345678nums = [int(t) for t in sys.stdin.readline().split()]#这一行的输出是[7,15,9,5]#我们现在把他拆解一下，如果写成这样nums = sys.stdin.readline().split()#现在这一行的输出变成了['7','15','9','5'],和之前的区别是现在list里每一个元素都是一个字符。#如果再改成这样nums = sys.stdin.readline()#现在的输出就变成了7 15 9 5，注意这是一个str，比如nums[3]，就会输出5，当然5也是一个字符。 例二输入：123454 43332323333322323 可以看到第一行的第一个4控制接下来输入的行数，第一行的第二个4控制接下来每一行的输入的元素。1234567import sysm, n = [int(x) for x in sys.stdin.readline().split()]nums = [[int(x) for x in sys.stdin.readline().strip()] for i in range(m)]#重点看一下第3行的代码,相当于用了2层for循环#外循环是运行m次int(x) for x in sys.stdin.readline().strip()，内循环是将每一行输入进来的str转换成int类型#这一行将来的输出是一个二维数组[[3, 3, 3, 2], [3, 2, 3, 3],[3, 3, 3, 2],[2, 3, 2, 3]] 目前还没看到别的不同的例子，如果今后碰到再补充。","raw":null,"content":null,"categories":[{"name":"code","slug":"code","permalink":"https://pspxiaochen.club/categories/code/"}],"tags":[{"name":"笔试","slug":"笔试","permalink":"https://pspxiaochen.club/tags/笔试/"}]},{"title":"K近邻法(KNN)之不再遗忘。","slug":"KNN","date":"2018-06-22T13:27:00.000Z","updated":"2018-06-23T01:52:15.078Z","comments":true,"path":"KNN/","link":"","permalink":"https://pspxiaochen.club/KNN/","excerpt":"","text":"K近邻算法的基本&emsp;&emsp; k近邻法(k-nearest neighbor,KNN)是一种可以做分类也可以做回归的方法。是一种比较简单的方法，给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最近的K个实例，这K个实例的多数属于某个类，就把该输入实例分到这个类。 基本过程：&emsp;&emsp; 输入：训练数据集T和实例特征向量x.&emsp;&emsp; 输出：实例x所属的类y.&emsp;&emsp; 过程：（1）根据给定的距离度量，在训练集T中找到与x最近的k个点，涵盖着k个点的集合记作N。&emsp;&emsp;&emsp;&emsp; &emsp;（2）在N中根据分类决策规则决定x的类别y,比如可以将N中类别最多的类别就定义为x的类别。 特殊情况：&emsp;&emsp; 当k = 1的情形，称为最近邻算法。对于输入的实例点x，他的分类结果就是距离他最近的训练数据中的点。&emsp;&emsp; 可以看出k近邻算法没有显式的学习过程。 K近邻模型的三个基本要素三个基本要素分别是：距离度量、K值的选择、分类决策规则。 距离度量：&emsp;&emsp; 特征空间中两个实例点的距离是两个实例点相似程度的反应。&emsp;&emsp; 若有两个实例$x_i$和$x_j$，这两个实例的$L_p$距离被定义为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L_p(x_i,x_j)=(\\sum_{i=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$&emsp;&emsp; p是&gt;=1的，当p=2时,我们称为欧式距离：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L_p(x_i,x_j)=(\\sum_{i=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\\frac{1}{2}}$&emsp;&emsp;当p=1时，我们称为曼哈顿距离&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L_p(x_i,x_j)=(\\sum_{i=1}^n|x_i^{(l)}-x_j^{(l)}|)$&emsp;&emsp;怎么通俗的理解曼哈顿距离和欧式距离呢&emsp;&emsp;假如你现在想从宿舍去食堂吃饭，你有2种走法第一种就是两点之间线段最近，直接走一条直线，管他有没有障碍物全部都挡不住你，你是电，你是光，你是唯一的神话，这就叫欧氏距离。&emsp;&emsp;还有一种走法就是你肯定得按照学校的规划道路上走，必须在路上，你不能穿墙，上天，入地。所以你可能会有好几种走法，但是都必须在路上走。这种就叫做曼哈顿距离。还有很多种距离，我只介绍这两种。&emsp;&emsp;距离度量的选择可能会影响之后的结果。 K值的选择：K值的选择会对K近邻法的结果产生重大的影响。&emsp;&emsp;先了解一下什么叫近似误差和估计误差&emsp;&emsp;近似误差：可以理解为对现有训练集的训练误差。&emsp;&emsp;估计误差：可以理解为对测试集的测试误差。 &emsp;&emsp;近似误差关注训练集，如果近似误差小了会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。模型本身不是最接近最佳模型。&emsp;&emsp;如果选择较小的k值，就相当于只有与输入实例较近（相似）的实例才会对预测结果起作用，‘学习’的近似误差会减小，估计误差会增大，如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，K值的减小意味着整体模型变的复杂，容易发生过拟合。 &emsp;&emsp;如果选择比较大的k值，就相当于用较多的训练实例进行预测。有点是可以减少学习的估计误差，缺点是学习的近似误差会增大。这时与输入实例较远（不相似）的训练实例也会对预测起作用，使预测发生错误。K值的增大就意味着整体的模型变得简单。&emsp;&emsp;如果K = N(样本总量) ，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。&emsp;&emsp;在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。 分类决策规则&emsp;&emsp;K近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。 KNN的优缺点优点1.简单，易于理解，易于实现，无需参数估计，无需训练，既可以用来做分类也可以用来做回归；2.可用于数值型数据和离散型数据；3.训练时间复杂度为O(n)；无数据输入假定；4.对异常值不敏感,kNN不会受到差别特别大的样本中的特征元素的影响(对异常值不敏感)。因为采用了归一化技术。 缺点：1.计算复杂性高；空间复杂性高；2.可解释性差，无法告诉你哪个变量更重要，无法给出决策树那样的规则；3.K值的选择：最大的缺点是当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进;4.最大的缺点是无法给出数据的内在含义。5.消极学习方法、懒惰算法。 伪代码 计算已知类别数据集中的点与当前点之间的距离（值域越大的变量常常会在距离计算中占据主导作用，需要进行归一化或者标准化处理）； 按照距离递增次序排序； 选择与当前距离最小的k个点； 确定前k个点所在类别的出现概率 ； 返回前k个点出现频率最高的类别作为当前点的预测分类。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"https://pspxiaochen.club/tags/KNN/"}]},{"title":"朴素贝叶斯之了断心结。","slug":"bys","date":"2018-05-30T13:27:00.000Z","updated":"2018-06-25T03:37:41.161Z","comments":true,"path":"bys/","link":"","permalink":"https://pspxiaochen.club/bys/","excerpt":"","text":"&emsp;&emsp; 为什么说贝叶斯一直是我的心结呢。这说来就话长了，当初考研时，想都没想就选择了相对简单的专硕（现在看来其实也并不简单），而我们学习的专硕是不考概率论的，所以基本上我的概率论体系还停留在高中阶段。那你又要问了，难道一个计算机科班出身的，本科就没学过概率论么，说到这里我就想起来我的概率论老师上课操着一口湖南口音，声音很小，150个人的大教室，而我在最后一排和舍友疯狂的搓着3DS，咳咳。说远了。今天我一定用最通俗理解的方法把朴素贝叶斯里的每一个知识点都梳理一遍，算是对自己有一个交代。 如果没什么概率论基础的话先推荐一篇文章应该如何理解概率分布函数和概率密度函数？ 生成式模型和判别式模型。&emsp;&emsp;在讲贝叶斯之前我觉得我有理由先说一下什么是生成式模型而什么是判别式模型。在《统计学习方法》书中明确写道：监督学习方法又可以分为生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。 生成式模型：&emsp;&emsp;生成方法由数据学习联合概率分布P(X,Y),然后求出条件概率分布P（Y|X）作为预测模型，即生成模型 P（Y|X）= P(X,Y) / P(X),这样的方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。我们今天的主角朴素贝叶斯就是生成模型，常见的还有隐马尔科夫模型。&emsp;&emsp;生成方法的特点：生成方法可以还原出联合概率分布P（X,Y），而判别方法则不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型。 判别式模型：&emsp;&emsp;判别方法由数据直接学习决策函数f(X)或者条件概率分布P（Y|X）作为预测的模型。判别方法关系的是对给定的输入X，应该预测什么样的输入出Y。&emsp;&emsp;判别方法的特点：判别方法直接学习的是条件概率P（Y|X）或者决策函数F（X），直接面对预测，往往学习的准确率更高。由于直接学习P（Y|X）或者F（X），可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 总结&emsp;&emsp;两种模型都是利用条件概率判别，只不过生成模型通过求解联合分布得到条件概率，而判别模型直接计算条件概率，如果你不知道什么是联合概率分布，我一会会解释，别怕。 现在开始我们的主菜（朴素贝叶斯）&emsp;&emsp;首先先声明一点，朴素贝叶斯与贝叶斯估计是不同的概念。那为什么叫朴素呢。是因为他假设特征条件都相互独立（不过一般都是不可能的）。所以说朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。&emsp;&emsp;X是定义在输入空间上的随机向量，Y是定义在输出空间上的随机变量。P（X,Y）是X和Y的联合概率分布。训练数据集由P（X,Y）独立同分布产生。&emsp;&emsp;朴素贝叶斯通过训练数据集学习联合概率分布P（X,Y），那我们现在说说什么是联合概率分布。 联合概率分布&emsp;&emsp;联合概率分布简称联合分布，是两个及以上随机变量组成的随机向量的概率分布。根据随机变量的不同，联合概率分布的表示形式也不同。对于离散型随机变量，联合概率分布可以以列表的形式表示，也可以以函数的形式表示；对于连续型随机变量，联合概率分布通过一非负函数的积分表示。打靶时命中的坐标（x，y）的概率分布就是联合概率分布（涉及两个随机变量），其他同样类比。 连续型联合概率分布&emsp;&emsp;对于二维连续随机向量，设X,Y为连续性随机变量，其联合概率分布或连续性随机变量（X,Y）的概率分布F（x,y）通过一非负函数f(x,y) &gt;= 0的积分表示，称函数f(x,y)为联合概率密度函数。两者关系如下：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 在接上面，我们想要通过训练数据集学习联合概率分布P（X,Y）= P(XY），而我们知道条件概率公式： P(AB)=P(A|B)P(B)=P(B|A)P(A) ，所以我们只要知道先验概率分布以及条件概率分布。就可以得到联合概率分布。&emsp;&emsp;朴素贝叶斯对条件概率分布做了条件独立性的假设。这个是较强的假设。具体的，条件独立性假设是：&emsp;&emsp;&emsp;&emsp;（1）这个公式可以这么理解：当事件A1，A2，A3相互独立时，有P(A1,A2,A3）= P（A1)P(A2)P(A3)p(A1,A2,A3|B) = P(A1|B)P(A2|B)P(A3|B) &emsp;&emsp;朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型计算后验概率分布P=（Y=c_k|X=x)，将后验概率最大的类作为x的类输出，后验概率计算根据贝叶斯定理进行,现在开始手推公式：由条件概率公式得到：$P(Y=c_k|X=x)=\\frac{p(X=x,Y=c_k)}{P(x)} = \\frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}$ （2） 再由全概率公式得到：$P(X=x) = \\sum_{k} P(Y=c_k)P(X=x|Y=c_k)$ （3） 将（3）带入（2）得到：$P(Y=c_k|X=x)=\\frac{p(X=x,Y=c_k)P(Y=c_k)}{\\sum_{k} P(Y=c_k)P(X=x|Y=c_k)}$ （4） 再将（1）带入（4）得到：$P(Y=c_k|X=x)=\\frac{P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}，k=1,2,3,K。$ 到此朴素贝叶斯分类器可以表示为：$y=f(x)=\\arg\\min_{c_k}\\frac{P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}$ 由于分母就是$P(X=x)$ 所以对于所有c_k来说都是相等的，所以可以把分母省略掉。所以最后的公式为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$y=f(x)=\\arg\\min_{c_k}{P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}$ 后验概率最大化的含义朴素贝叶斯将实例分到后验概率最大的类中，也就是希望损失函数尽量的小，损失函数越小，模型越好，只需所有的样本点都求一次损失函数然后进行累加就是经验风险，我们希望让经验风险也尽可能的越小越好，经验风险是对训练集中的所有样本点损失函数的平均最小化。经验风险越小说明模型f(X)对训练集的拟合程度越好，但是对于未知的样本效果怎么样呢？我们知道未知的样本数据（X,Y）的数量是不容易确定的，所以就没有办法用所有样本损失函数的平均值的最小化这个方法，那么怎么来衡量这个模型对所有的样本（包含未知的样本和已知的训练样本）预测能力呢？熟悉概率论的很容易就想到了用期望。这下我们又得再来说说数学期望了，数学期望想必都比较数学，我只说一下离散型和连续型求法的区别。 离散型：$E(X)=\\sum_{i}x_ip_i$ 连续型：$E(X)=\\int_{-\\infty}^{+\\infty} {xf(x)} \\,{\\rm d}x$&emsp;&emsp;&emsp;&emsp;&emsp;$f(x)$为概率密度函数。 假设我们的损失函数是0-1损失函数：&emsp;&emsp;&emsp;&emsp;设X和Y服从联合分布P(X,Y).那么期望风险就可以表示为：$R_{exp}(f) = E[L(Y,f(x))]$接着推公式得到：$R_{exp}(f) = E[L(Y,f(x))]$可以看出最后变成了条件期望。&emsp;&emsp;这就是期望风险，期望风险表示的是全局的概念，表示的是决策函数对所有的样本预测能力的大小，而经验风险则是局部的概念，仅仅表示决策函数对训练数据集里样本的预测能力。理想的模型（决策）函数应该是让所有的样本的损失函数最小的（也即期望风险最小化），但是期望风险函数往往是不可得到的，即上式中，X与Y的联合分布函数不容易得到。现在我们已经清楚了期望风险是全局的，理想情况下应该是让期望风险最小化，但是呢，期望风险函数又不是那么容易得到的。怎么办呢？那就用局部最优的代替全局最优这个思想吧。这就是经验风险最小化的理论基础。&emsp;&emsp;为了使条件期望最小化，只需要对X=x每一个都极小化： 根据期望风险最小化准则就得到了后验概率最大化准则。 总的来说 朴素贝叶斯就是把朴素的思想（条件独立性）带入到贝叶斯公式之中，来得到朴素贝叶斯分类器。中间需要计算先验概率和条件概率，然后通过极大似然估计可以估计这些参数。 最后在推荐2篇文章，里面写的十分详细。数学之美番外篇：平凡而又神奇的贝叶斯方法贝叶斯推断及其互联网应用（一）：定理简介","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"贝叶斯","slug":"贝叶斯","permalink":"https://pspxiaochen.club/tags/贝叶斯/"}]},{"title":"我自己理解的梯度下降的原理","slug":"2018-05-24-GD","date":"2018-05-23T13:27:00.000Z","updated":"2018-07-05T09:38:04.650Z","comments":true,"path":"2018-05-24-GD/","link":"","permalink":"https://pspxiaochen.club/2018-05-24-GD/","excerpt":"","text":"&emsp;&emsp;最近不太顺啊，各种碰壁，看来学习确实容不得半点虚假，会就是会，不会就是不会。现在决定再认认真真复习一遍机器学习和数据结构，再来巩固一下自己的知识体系，只希望这次可以别忘的太快。 我们为什么要使用梯度下降。&emsp;&emsp;因为机器学习总的来说是一个优化问题。我们有一个想要优化的函数，比如说损失函数，我们总是希望可以让损失函数变得越来越小。那怎么才能使损失函数变得原来越小呢？损失函数中有一些未知的参数，我们只要不停更新这个参数，就可以让损失函数越来越小。直到损失函数等于0或者不再发生变化为止。所以说梯度下降是用来更新一些未知参数的，而这些参数会使损失函数越来越小。 梯度下降是怎么来的呢？&emsp;&emsp;首先我们来看一元函数的泰勒展开，以便于更好的理解多元函数的泰勒展开。如果一个一元函数n阶可导，它的泰勒展开公式为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;如果在某一点处的导数值大于0（+），则函数在此处是增函数，加大x的值函数会增加，减小x的值函数会减小。相反的，如果在某一点处导数值小于0（-），则函数是减函数，增加x的值函数值会减小（+），减小x的值函数会增加。因此我们可以得出一个结论：如果x的变化很小，并且变化值与导数值反号，则函数值下降。对于一元函数，x的变化只有两个方向，要么朝左，要么朝右。 &emsp;&emsp;下面我们把这一结论推广到多元函数的情况。多元函数f(x)在x点处的泰勒展开为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;这里我们忽略了二次以及更高的项。其中一次项是梯度向量▽f(x)与自变量的增量△X的內积，这等价于一元函数的一次项f’(X0)(X-X0)。这样，函数的增量与自变量的增量△X、函数梯度的关系可以表示为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;如果△X足够小，在X的某一领域内，则我们可以忽略二次及以上的项，有：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;这里的情况比较复杂，△X是一个向量，有无穷多种方向，该往哪个方向走呢？如果能保证上面约等式的右边恒小于0，则有：f(x+△x) &lt; f(x)，即函数值递减，这就是下山的正确方向。如果这句话还不能理解，我们可以再换一种方式理解。假如x是我们想要更新的参数,f(x)是我们刚开始需要优化的损失函数,我们很希望可以让f(x)越来越小，那怎么才能够让其越来越小呢。就如我一开始说的那样我们要不停的更新里面的参数,而x就是我们要更新的那个参数。用数学表达式就是min(f(x+△x)），如果f(x+△x) &lt; f(x)恒成立，那就是说，我们的优化函数一直再减小，那么可以理解为一定会找到一个完美的结果，是不是想想就有点小开心呢，但是往往不会有那么好的事情的，就和人生一样。&emsp;&emsp;再接上面，那我们怎么样才能保证约等式的右边恒小于0呢，因为有：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;在这里，||·||表示向量的模，θ是向量▽f(x)和△x的夹角。因为向量的模一定大于等于0，所以说如果cosθ &lt;= 0,则能保证约等式的右边恒小于0。也就是选择合适的增量△x，就能保证函数数值下降，要达到这种效果，只要保证梯度和△x的夹角的余弦值小于等于0就行。由于cosθ &gt;= -1，当θ = π时，cosθ有极小值-1，此时梯度和△x反向，夹角为180度，因此当向量△X的模大小一定时，当：△X = - ▽f(x),也就是在梯度相反的方向函数值下降最快。此时有cosθ = -1。重点来了：那我们为什么要让△X = - ▽f(x)呢，让它等于别的不可以吗？这一块我考虑了好半天，最后我只能这么解释一下。如果我们想让约等式的右边小于等于0恒成立，怎么办！！怎么样才能保证最靠谱呢？那我不如让△X = - ▽f(x)算了，一个平方的值带负号是一定小于等于0的。这也太靠谱了吧！！我觉得这么取得意义就在这里，仅此而已。&emsp;&emsp;也就是说只要梯度不为0，往梯度的反方向走函数值一定是下降的。直接用△X = - ▽f(x)可能会有问题，因为x+△x可能会超出x的领域范围之外，此时是不能忽略泰勒展开中的二次及以上的项的，因此步伐不能太大，一般设△x = -α▽f(x)。其中α为一个接近于0的正数，称为步长，由人工设定。&emsp;&emsp;从初始点X_0开始，使用此迭代公式：X_k+1 = X_k - α▽f(x).&emsp;&emsp;只要没有到达梯度为0的点，则函数值会沿着序列X_k递减，最终会收敛到梯度为0的点，这就是梯度下降法。迭代终止的条件是函数的梯度值为0（实际是接近于0），此时认为已经达到了极值点。注意我们找到的是梯度为0的点，这不一定就是极值点。梯度下降法只需要计算函数在某些点处的梯度，实现简单，计算量小。&emsp;&emsp;再写一套比较直观的公式推导，看完有点原来如此，so easy的感觉~~&emsp;&emsp;$f(x_{k+1}) = f(x_k + x_{k+1} - x_k) = f(x_k) + ▽f(x_k)(x_{k+1} - x_k)$&emsp;&emsp;$▽f(x_k)$即为函数f在$x_k$处的梯度。&emsp;&emsp;为了使$f(x_{k+1}) &lt; f(x_k)$,则需要$▽f(x_k)(x_{k+1} - x_k)&lt;0$&emsp;&emsp;则只需要使$x_{k+1}=x_k - \\gamma▽f(x_k)$即可。&emsp;&emsp;上式便是梯度下降的迭代公式，其中$\\gamma$为学习速率。这样看是不是很清楚了呢。。。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"梯度下降","slug":"梯度下降","permalink":"https://pspxiaochen.club/tags/梯度下降/"}]},{"title":"从拉格朗日乘子法再到KKT条件的理解","slug":"lagrange and kkt","date":"2017-12-26T13:27:00.000Z","updated":"2017-12-26T13:31:10.714Z","comments":true,"path":"lagrange and kkt/","link":"","permalink":"https://pspxiaochen.club/lagrange and kkt/","excerpt":"","text":"&emsp;&emsp;西瓜书看了有一阵子了，终于看到了SVM，遥想研一看SVM的时候。。我操 这是什么。。这又是什么。。论高中数学的重要性的，出来混都是要还的。只好一点一点补齐SVM中用到的各种知识点。今天先扯一扯自己对拉格朗日乘子法和KKT条件的理解，希望这篇文章写完之后，能让我对这2个臭逼玩意能有更好的理解，那么现在开始。 我们为什么在SVM中要用到拉格朗日和KKT条件！！！&emsp;&emsp;因为在SVM中我们要计算一个有约束条件的极值问题，而拉格朗日乘子法和KKT条件是非常重要的两个求取方法。&emsp;&emsp;对于等式约束的优化问题（就是限制的那个式子是个等式 s.t h(x) = 0 类似这种），可以应用拉格朗日乘子法去求取最优值。&emsp;&emsp;如果含有不等式约束，可以应用KKT条件去求取最优值。&emsp;&emsp;这两种方法求得的结果只是必要条件，只有当是凸函数（二次导数大于0为凸）的情况下，才能保证是充分必要条件。而KKT条件是拉格朗日乘子法的泛化。&emsp;&emsp;下面我就针对这两个玩意，谈一谈我的理解。 拉格朗日乘子法！！！&emsp;&emsp;拉格朗日乘子法在考研的数学二中就用到过，当时只知道要那么用，那么用就能解决条件极值的问题，但是真心不知道为什么，这次先把为什么可以用拉格朗日说一下。&emsp;&emsp;想象一下，目标函数f(x,y)是一座山的高度，约束g(x,y)=C是镶嵌在山上的一条曲线如下图。（渣画技看看就好了）&emsp;&emsp;你为了找到曲线上的最低点，就从最低的等高线（0那条）开始网上数。数到第三条，等高线终于和曲线有交点了（如上图所示）。因为比这条等高线低的地方都不在约束范围内，所以这肯定是这条约束曲线的最低点了。&emsp;&emsp;而且约束曲线在这里不可能和等高线相交，一定是相切。因为如果是相交的话，如下图所示，那么曲线一定会有一部分在B区域，但是B区域比等高线低，这是不可能的。假设g(x)与等高线相交，交点就是同时满足等式约束条件和目标函数的可行域的值，但肯定不是最优值，因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小，只有到等高线与目标函数的曲线相切的时候，可能取得最优值。&emsp;&emsp;两条曲线相切，意味着他们在这点的法线平行，也就是法向量只差一个任意的常数乘子（取为-λ）：&emsp;&emsp;∇f（x, y) = -λ(∇g(x,y)-C) 把这个式子的右边移动到左边，就得到∇（f(x,y)+λ(g(x,y)-C)=0&emsp;&emsp; 在看看这个式子，你又能想起来什么呢？高中数学。。。这个就是f(x,y)+λ(g(x,y)-C)没有约束情况下极值点的充分条件吧。。（令其导数等于0，求极值点。。。高中用的就是这个套路。。哎）所以证明了拉格朗日乘子法确实可以解决这类问题，拉格朗日确实牛逼。。高中数学也应该好好学 哎。难受啊。&emsp;&emsp;那拉格朗日乘子法到底是怎么用的呢？简单的再说一下就是 把等式约束hi(x)用一个系数与f(x)写为一个式子，称为拉格朗日函数，而系数称为拉格朗日乘子。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。 KKT条件！！！！！&emsp;&emsp;我先给出KKT条件：&emsp;&emsp;对于具有等式和不等式约束的一般优化问题：&emsp;&emsp;&emsp;&emsp;KKT条件给出了判断X*是否是最优解的必要条件：&emsp;&emsp; 不等式约束优化问题&emsp;&emsp;我们先给出其主要思想：转化的思想——将不等式约束条件变成等式约束条件.具体做法：引入松弛变量.松弛变量也是优化变量，也需要一视同仁求偏导.&emsp;&emsp;具体来说，我们先看一个一元函数的例子：&emsp;&emsp;&emsp;&emsp;min f(x)&emsp;&emsp;s.t. g1(x) = a - x ≤ 0&emsp;&emsp;&emsp;&emsp;g2(x) = x - b ≤ 0&emsp;&emsp;（注：优化问题中，我们必须求得一个确定的值，因此不妨令所有的不等式均取到等号，即≤的情况.）&emsp;&emsp;对于约束g1和g2,我们分别引入两个松弛变量a₁²和b₁²,得到h1(x,a₁) = g1 + a₁² = 0和h2(x,b₁) = g2 + b₁² = 0,这里直接加上平方项a₁²和b₁²而非a₁和b₁,是因为g1和g2这两个不等式的左边必须加上一个正数（或者0）才能使不等式变成等式。若只加上a₁和b₁,又会引入新的约束a₁≥0,b₁≥0.这岂不是很爆炸！！！&emsp;&emsp;由此我们将不等式约束转化为了等式约束，并得到了拉格朗日函数L（x,a₁,b₁,u₁,u₂) = f(x) + u₁(a - x + a₁²) + u₂(x - b + b₁²)&emsp;&emsp;我们再按照等式约束优化问题对齐求解，联立方程组：（注：这里的u₁ ≥ 0,u₂ ≥ 0先承认，等会再解释，实际上对于不等式约束前的乘子，我们要求其大于等于0）得出方程组后，便开始手动解，先看第3行的两个式子 u₁a₁ = 0 和 u₂b₁ = 0比较简单，我们就从他们入手。对于u₁a₁ = 0,我们有两种情况:情形1: u₁ = 0,a₁ ≠ 0:此时由于乘子u₁ = 0,因此g1与其相乘为0，可以理解为约束g1不起作用，且有g1(x) = a - x &lt; 0 (因为a₁ ≠ 0 , 所以他的平方一定是大于0的）情形2：u₁ ≥ 0 , a₁ = 0此时，g1(x) = a - x = 0 且 u1 &gt; 0 (因为a₁ = 0 为了 a - x + a₁² = 0 ， 所以 a - x必须等于0），可以理解为约束g1起作用了，且有g1(x) = 0 合并两种情况的：u1g1 = 0 且在约束起作用时,u₁ &gt; 0,g1(x) = 0;约束不起作用时u1 = 0,g1(x) &lt; 0同样的，分析u₂b₁ = 0,可得出约束g2起作用和不起作用的情形，并分析得到u₂g₂ = 0由此，方程组（极值必要条件）转化为：这是一元一次的情形，类似的，对于多远多次不等式约束问题min f(x)s.t.g_j(x) ≤ 0 ( j = 1,2,````,m)我们有上式便称为不等式约束优化问题的KKT（Karush-Kuhn-Tucker）条件u_j称为KKT乘子，且约束起作用时u_j＞0,g_j(x) = 0;约束不起作用时u_j = 0,g_j(x) &lt; 0. 总结：同时包含等式和不等式约束的一般优化问题注意，对于等式约束的Lagrange乘子，并没有非负的要求！以后求其极值点，不必再引入松弛变量，直接使用KKT条件判断！","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://pspxiaochen.club/tags/SVM/"}]},{"title":"偏差与方差的总结","slug":"2017-12-12-bias-var","date":"2017-12-12T13:27:00.000Z","updated":"2017-12-26T03:11:52.843Z","comments":true,"path":"2017-12-12-bias-var/","link":"","permalink":"https://pspxiaochen.club/2017-12-12-bias-var/","excerpt":"","text":"&emsp;&emsp;今天开始重新仔细学习西瓜书（机器学习），今天看完了第一章，第一章基本都是一些概念的东西，公式也比较少，希望可以坚持把这本书看完。 基本概念&emsp;&emsp;根据各种公式推导得知，泛化误差 = 偏差 + 方差 +噪声&emsp;&emsp;而偏差（bias）度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。方差(Variance)度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。 偏差与方差的关系&emsp;&emsp;偏差与方差是有冲突的。给定学习任务，假定我们能控制学习算法的训练程度，则在训练不足时，模型的拟合能力不够强，训练数据的扰动不足以使模型产生显著变化，此时偏差主导了泛化错误率；随着训练程度的加深，模型的拟合能力逐渐增强，训练数据发生的扰动渐渐能被模型学习到，方差逐渐主导了泛化错误率；在训练程度充足后，模型的拟合能力已经非常强，训练数据发生的轻微扰动都会导致模型发生显著变化，若训练数据自身的，非全局性的特性被模型学习到了，则将发生过拟合。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"偏差","slug":"偏差","permalink":"https://pspxiaochen.club/tags/偏差/"},{"name":"方差","slug":"方差","permalink":"https://pspxiaochen.club/tags/方差/"}]},{"title":"Sum of Two Integers（使用位运算实现）","slug":"2017-12-07-add","date":"2017-12-07T08:27:00.000Z","updated":"2017-12-07T08:37:43.607Z","comments":true,"path":"2017-12-07-add/","link":"","permalink":"https://pspxiaochen.club/2017-12-07-add/","excerpt":"","text":"思路&emsp;&emsp;两个数的加法分为两步，对应位相加和进位。&emsp;&emsp;举个简单的例子：997+24 &emsp;&emsp;我们平时计算时是将对应位相加和进位同时计算，其实可以保留下进位，只计算对应位相加，保留进位的位置（值）。接下来，将进位向左移动一位，将上一步的结果与移位后的进位值进行对应位相加，直到没有进位结束。 &emsp;&emsp;对于二进制数的而言，对应位相加就可以使用异或（xor）操作，计算进位就可以使用与（and）操作，在下一步进行对应位相加前，对进位数使用移位操作（&lt;&lt;）。 &emsp;&emsp;这样就非常好理解下面的实现代码。 12345678910int getSum(int a, int b)&#123; while (b) &#123; int c = a ^ b; b = (a &amp; b) &lt;&lt; 1; a = c; &#125; return a; &#125; &emsp;&emsp;最后，再给一个详细的运行过程示意，计算523+1125.（另外，如果是有负数的话，算法也是可行的，可以去看一下补码的相关内容）","raw":null,"content":null,"categories":[{"name":"刷题","slug":"刷题","permalink":"https://pspxiaochen.club/categories/刷题/"}],"tags":[{"name":"位运算","slug":"位运算","permalink":"https://pspxiaochen.club/tags/位运算/"},{"name":"异或","slug":"异或","permalink":"https://pspxiaochen.club/tags/异或/"}]},{"title":"峰度（Kurtosis）和偏度（Skewness）","slug":"2017-11-28-Kurtosis","date":"2017-11-28T11:12:00.000Z","updated":"2017-11-28T13:24:18.674Z","comments":true,"path":"2017-11-28-Kurtosis/","link":"","permalink":"https://pspxiaochen.club/2017-11-28-Kurtosis/","excerpt":"","text":"峰度（Kurtosis）&emsp;&emsp;峰度是描述总体中所有取值分布形态陡缓程度的统计量。这个统计量需要与正态分布相比较，峰度为0表示该总体数据分布与正态分布的陡缓程度相同；峰度大于0表示该总体数据分布与正态分布相比较为陡峭，为尖顶峰；峰度小于0表示该总体数据分布与正态分布相比较为平坦，为平顶峰。峰度的绝对值数值越大表示其分布形态的陡缓程度与正态分布的差异程度越大。&emsp;&emsp;峰度的具体计算公式为：&emsp;&emsp; 偏度（Skewness）&emsp;&emsp;偏度与峰度类似，它也是描述数据分布形态的统计量，其描述的是某总体取值分布的对称性。这个统计量同样需要与正态分布相比较，偏度为0表示其数据分布形态与正态分布的偏斜程度相同；偏度大于0表示其数据分布形态与正态分布相比为正偏或右偏，即有一条长尾巴拖在右边，数据右端有较多的极端值；偏度小于0表示其数据分布形态与正态分布相比为负偏或左偏，即有一条长尾拖在左边，数据左端有较多的极端值。偏度的绝对值数值越大表示其分布形态的偏斜程度越大。&emsp;&emsp;偏度的具体计算公式为：&emsp;&emsp;","raw":null,"content":null,"categories":[{"name":"其他","slug":"其他","permalink":"https://pspxiaochen.club/categories/其他/"}],"tags":[{"name":"Kurtosis","slug":"Kurtosis","permalink":"https://pspxiaochen.club/tags/Kurtosis/"}]},{"title":"Hexo博客撰写之：MarkDown语法介绍","slug":"2017-11-17-test","date":"2017-11-17T16:00:00.000Z","updated":"2017-11-20T08:03:16.896Z","comments":true,"path":"2017-11-17-test/","link":"","permalink":"https://pspxiaochen.club/2017-11-17-test/","excerpt":"","text":"前言搭建好了hexo，接下来的就是写博客了， hexo 支持用 markdown 写博客，markdown 语法很简单，本文给出一些基本的 markdown 语法，结合 hexo 教你如何在 hexo 下用 markdown 撰写博客。 hexo博客头部hexo 博客的 markdown 头部有固定的格式，如下所示： 1234567891011---title: Hexo博客撰写之：MarkDown语法介绍date: 2017-11-17 09:12:00categories: - 其他tags: - hexo - markdowndescription: 本文介绍如何在hexo搭建的博客下用markdown写文章,以及一些markdown的基本语法．photos: - https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike150%2C5%2C5%2C150%2C50/sign=62597ff6ba51f819e5280b18bbdd2188/908fa0ec08fa513d7ddc9503376d55fbb3fbd9fd.jpg 顾名思义，我们很容易看懂这个头部信息．我们写新博客的时候只需要复制这些内容，然后对相应的内容进行修改，但不要修改格式． hexo博客正文hexo 博客正文就是正常的 markdown 了，下面就介绍一些基本的 markdown 语法． 基本的markdown语法 标题通过在行首插入 1 到 6 个 # ，来定义 1 到 6 阶 标题： Markdown 预览 # 一级标题 #一级标题 ## 二级标题 ## 二级标题 ### 三级标题 ### 三级标题 段落和换行 在 Markdown 中段落由一行或者多行文本组成，相邻的两行文字会被视为同一段落，如果存在空行则被视为不同段落( Markdown 对空行的定义是看起来是空行就是空行，即使空行中存在 空格 TAB 回车 等不可见字符，同样会被视为空行)。 Markdown 预览 第一行相邻被视为统一段落 第一行 相邻被视为同一段落 第一行[空格][空格]上一行结尾存在两个空格，段内换行 第一行上一行结尾存在两个空格，段内换行 第一行两行之间存在空行，视为不同段落。 第一行两行之间存在空行，视为不同段落。 缩进 输入法中文全角状态下输入两个空格即可实现缩进．输入两个&amp;emsp;或&amp;ensp;也可以实现空格缩进． 强调 Markdown 预览 *倾斜* 倾斜 **粗体** 粗体 ~~删除线~~ 删除线 &gt;引用 &gt;引用 连接和图片 Markdown 预览 [百度一下](http://www.baidu.com) 百度一下 ![joker](https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1511008391023&amp;di=1ff315398f4671c899dc706aa34693de&amp;imgtype=0&amp;src=http%3A%2F%2Fimg3.duitang.com%2Fuploads%2Fitem%2F201203%2F17%2F20120317230833_kvzj3.thumb.700_0.jpeg) 音频和视频可以直接使用html的 和 标签嵌入 音频 和 视频．比如以下视频标签：Markdown:12345 &lt;center&gt; &lt;video width=320 height=200 src='http://tb-video.bdstatic.com/tieba-smallvideo/32_054ab95c5e7edd7a22c913fd1d1d8a5c.mp4' controls='controls' &gt; 您的浏览器不支持 video 标签。 &lt;/video&gt;&lt;/center&gt; 列表 无序列表用 - 引领列表内容， 有序列表用 数字 引领列表内容， 需要指出的是：有序列表的数字即便不按照顺序排列，结果仍是有序的。 下划线和特殊符号 由于 Markdown 使用一些特殊符号进行标记，当我们想要在文档中使用这些特殊符号并防止被 Markdown 转换的时候，可以使用 \\ (转义符) 将这些特殊符号进行转义。 Markdown 预览 在一行中用三个以上的减号来建立一个分隔线—- —- 可以利用反斜杠(转义字符)来插入一些在语法中有特殊意义的符号\\*Hi\\* *Hi* 代码 1 行内代码&emsp;行内代码可以使用反引号来标记(反引号一般位于键盘左上角，要用英文． Markdown 预览 一句话`行内代码`一句话 一句话行内代码 就比如` &lt; video &gt; `标签 就比如&lt; video &gt;标签 2 多行代码&emsp;多行代码使用 3 个反引号来标记(反引号一般位于键盘左上角，要用英文) ，在第一个 1234567Markdown:``` javascript ```javascript // 我是注释 var a = 5 ; console.log(a) 123456预览：```javascript// 我是注释var a = 5 ;console.log(a) 表格Markdown: | 默认 | 靠右 | 居中 | 靠左 || —— | —-: | :—: | :—- || 内容 | 内容 | 内容 | 内容 || 内容 | 内容 | 条目 | 内容 | 预览 默认 靠右 居中 靠左 内容 内容 内容 内容 内容 内容 条目 内容 OK先到这里,基本语法就是这样，以后如果学到新的好用语法了，我会继续推送的^_^","raw":null,"content":null,"categories":[{"name":"其他","slug":"其他","permalink":"https://pspxiaochen.club/categories/其他/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"https://pspxiaochen.club/tags/markdown/"},{"name":"hexo","slug":"hexo","permalink":"https://pspxiaochen.club/tags/hexo/"}]},{"title":"第一篇post：庆祝博客搭建成功","slug":"2017-11-20-first-post","date":"2017-11-16T16:00:00.000Z","updated":"2017-11-20T02:30:36.777Z","comments":true,"path":"2017-11-20-first-post/","link":"","permalink":"https://pspxiaochen.club/2017-11-20-first-post/","excerpt":"","text":"&emsp;&emsp;&ensp;我听中文音乐很少，先放一首比较喜欢的中文歌曲… &emsp; 《平凡之路》—-朴树 前言&emsp;&emsp;&ensp;一直想搭建自己的博客，但是一直没实践，只怪自己太懒。这次终于下定决心去实践，不过像我等小白来来回回折腾了好几天，本来想省点事直接在github上小改一下，谁知道老是出问题，一提交就收到Page build failure邮件，头疼的我差点放弃，最后狠下心来在本地安装了jekyll，一边调试一边修改，总算调通了，然后push到github，刷新主页，-_-，404找不到页面！妈呀，搜了好久解决办法，最后按照网上说的bundle update升级到了和github一样的最新版，这次报错了：MethedError：not find methed to_liquid for…想了半天，当前’github-pages’包的最新版是155,我把Gemfile中的’github-pages’改成了’github-pages’, ‘~&gt; 154’，也就是从默认的当前最新版本155降到了154,运行bundel exec jekyll serve，完美通过。。。我就郁闷了，原来这个jekyll-next主题已经不适用最新版的github-pages了。那怎么办呢，直接用hexo+next吧，索性静态页面就静态到底吧。 安装node和hexo（基于windows） 安装node去nodejs官网下载32或者64位的 node 安装包，然后在Windows下安装 node ，安装完成后，添加 node 到系统 PATH 变量，然后 Win+r 打开运行窗口，输入 cmd 打开命令窗口，然后键入： 1node -v 查看node是否已经安装好,再键入 1npm -v 安装Hexo这里先安装cnpm，以加快npm包的下载速度： 1npm install -g cnpm --registry=https://registry.npm.taobao.org 然后，安装hexo 1$ cnpm install hexo -g 打开cmd命令窗口,键入: 1hexo -v 查看hexo是否已安装好 安装git&emsp;&emsp;&ensp;去git for windows下载32或者64位的 git 安装包，然后在Windows下安装 git ，安装完成后，添加 git 到系统 PATH 变量，然后 Win+r 打开运行窗口，输入 cmd 打开命令窗口，然后键入：1git -v 查看git是否已经安装好 本地生成SSH key并添加到github 本地生成ssh keyhttps每次push需要输入用户名和密码，为了以后部署方便，我们使用ssh提交，使用ssh需要配置添加SSH key，具体如下：打开 git bash，输入以下命令: 12$ cd ~$ ssh-keygen -C \"your_computer_name\" 接着会提示输入文件名，默认就行了，Enter再接着会提示你输入两次密码，这个是push时候的密码，我们选择空密码，Enter没问题的话就成功了。 添加ssh key 到github 1$ clip &lt; ~/.ssh/id_rsa.pub 然后登录github，进入右上角Account Settings，然后点击菜单栏的SSH key进入页面添加key，点击Add SSH key按钮，把复制的SSH key代码粘贴到key所对应的输入框，点击确认，Title会默认使用你的”your_computer_name”。 测试该SSH key 1$ ssh -T git@github.com 出现 12$ Hi \" your-github-username \"! You've successfully authenticated, but GitHub does not provide shell access.$ Connection to github.com closed. ok,搞定。 搭建博客 新建github pages仓库注册github账号然后新建一个仓库，仓库名称为 your-github-username.github.io，比如我的是pspxiaochen.github.io 搭建博客在本地磁盘新建一个blog文件夹，比如在D盘新建一个blog文件夹，然后进入blog文件夹，执行以下操作：右键打开 git bash，输入以下命令 123$ git clone git@github.com:spaceJmmy/spaceJmmy-blog-template$ cd spaceJmmy-blog-template$ cnpm install 下载完成后,继续输入： 12$ hexo clean$ hexo s 如果出现 12INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop 说明启动成功，但是信息还是我的，所以接下来要修改配置博客了。 配置博客修改站点配置文件 spaceJmmy-blog-template/_config.yml： 修改站点信息，将以下内容改成你自己的信息： 12345# Sitetitle: spaceJmmy的博客 #博客名subtitle: 纯真容易幸福，单纯就易满足 #博客副标题description: #给搜索引擎看的，对站点的描述，可以自定义author: spaceJmmy #作者名称 修改站点 URL ，将站点 URL 改成你自己的 URL： 1234# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://pspxiaochen.github.ioroot: / 修改部署备份信息：把两个 git@github.com:spaceJmmy/spaceJmmy.github.io.git 换成成你自己的 repo 地址。 1234567891011# Deploymentdeploy: type: git repo: github: git@github.com:spaceJmmy/spaceJmmy.github.io.git,master message: updated at &#123;&#123; now(\"YYYY-MM-DD HH:mm:ss\") &#125;&#125; backup: type: git repository: github: git@github.com:spaceJmmy/spaceJmmy.github.io.git,src message: updated at &#123;&#123; now(\"YYYY-MM-DD HH:mm:ss\") &#125;&#125; 修改next主题配置文件 spaceJmmy-blog-template/themes/next/_config.yml： 修改 github 社交信息，将我的 GitHub 链接 https://github.com/spaceJmmy 改成你自己的链接：123social: #LinkLabel: Link GitHub: https://github.com/spaceJmmy 更换站点图标和用户头像： 更换站点图标更换本地文件夹 spaceJmmy-blog-template/themes/next/source 下面的 favicon.ico ，换成你自己的站点图标，文件名不要改变。 更换用户头像更换本地文件夹 spaceJmmy-blog-template/themes/next/source/images 下面的 avatar.gif ，换成你自己的用户头像，文件名不要改变。 修改关于页面： 修改文件夹 spaceJmmy-blog-template/source/about 下的 index.md 文件，改为你自己的 关于 页面。 测试配置是否成功 在git bash中输入以下命令： 12$ hexo clean $ hexo s 浏览器打开 http://localhost:4000/ ，如果成功的话，你会发现你的博客已经呈现出你的信息了，吼吼，狂欢吧…… 不过，先别急，先把网站部署备份了再说： OK，接下来部署备份你的网站，这时候在 bash 终端 Ctrl+C 停止服务器运行，然后输入： 1$ hexo d 你会发现静态网站已经 push 到你 repo 的 master 分支了。浏览器打开 your-github-username.github.io 就能看到你的博客了，哈哈…… 继续，备份博客源码之前需要先删除当前目录下的 .git 文件夹，然后 bash 输入： 1hexo b 你会发现网站源码已经备份到你 repo 的 src 分支了，至此，可以开心的庆祝啦，哈哈。 博客以后的常态化管理 以后写博客只需要自己写一个 .md 文件，然后放到/source/_posts文件夹下，写好博客后，来个拉风的部署三部曲，呼呼： 123$ hexo clean #清空缓存$ hexo d #部署站点到master分支$ hexo b #备份站点源代码到src分支 换台电脑重新部署（记得添加新的SSH key） 得益于前面的工作，换台电脑我们只需要clone仓库的src分支，然后重新生成hexo博客环境来撰写和发布post。 123$ git clone -b src git@github.com:your-github-username/your-github-username.github.io.git$ cd your-github-username.github.io$ cnpm install hexo环境搭建成功，然后 hexo s 本地预览，添加新的post，再按上述部署三部曲走起，呼呼… 有时 hexo b 会报错，提示执行 git push，那就 git push，你会看到 push 成功，哈哈。 &emsp;&emsp;&ensp;至此大功告成，看着自己现在这个博客上线，心里确实美滋滋啊，haha。 &emsp;&emsp;&ensp;这个博客的搭建，要感谢很多人… 首先感谢github，提供了git pages来托管我们的博客，而且是免费的; 然后要感谢提供主题模板的开源贡献者，使得像我这样的小白能够用上这么高大上的博客; 最后要感谢我自己，能够下定决心克服搭建博客的困难，谁让我是小白呢，慢慢进步。。。","raw":null,"content":null,"categories":[{"name":"其他","slug":"其他","permalink":"https://pspxiaochen.club/categories/其他/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://pspxiaochen.club/tags/hexo/"},{"name":"教程","slug":"教程","permalink":"https://pspxiaochen.club/tags/教程/"},{"name":"node","slug":"node","permalink":"https://pspxiaochen.club/tags/node/"},{"name":"git","slug":"git","permalink":"https://pspxiaochen.club/tags/git/"}]}]}