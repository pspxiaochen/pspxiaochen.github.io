{"meta":{"title":"发条晨","subtitle":"愿我走出半生，归来仍是少年","description":null,"author":"pspxiaochen","url":"https://pspxiaochen.club"},"pages":[{"title":"一个平凡的人","date":"2017-08-18T15:28:22.000Z","updated":"2018-07-07T09:34:15.497Z","comments":true,"path":"about/index.html","permalink":"https://pspxiaochen.club/about/index.html","excerpt":"","text":"这里言论自由个人邮箱: pspxiaochen@gmail.com索尼 任天堂 美剧 电影 阅读 旅游 乔治奥威尔 反乌托邦 锵锵三人行 圆桌派 新西兰","raw":null,"content":null},{"title":"categories","date":"2017-08-18T15:28:08.000Z","updated":"2017-11-20T07:24:06.654Z","comments":true,"path":"categories/index.html","permalink":"https://pspxiaochen.club/categories/index.html","excerpt":"","text":"","raw":null,"content":null},{"title":"tags","date":"2017-08-18T15:24:46.000Z","updated":"2017-11-20T07:24:06.656Z","comments":true,"path":"tags/index.html","permalink":"https://pspxiaochen.club/tags/index.html","excerpt":"","text":"","raw":null,"content":null}],"posts":[{"title":"决策树的整理（二）-----剪枝操作","slug":"decision-tree2","date":"2018-08-01T11:34:36.000Z","updated":"2018-08-02T09:24:38.081Z","comments":true,"path":"decision-tree2/","link":"","permalink":"https://pspxiaochen.club/decision-tree2/","excerpt":"","text":"&emsp;&emsp;决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但是对未知的测试数据的分类却不准确，这就叫过拟合，我们需要对决策树进行简化。这个过程叫做剪枝。 剪枝&emsp;&emsp;剪枝从已生成的树上裁掉一些子树或者叶节点，并将其根节点或父节点作为新的叶节点，从而简化分类树模型。 过程&emsp;&emsp;决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶子结点个数为|T|，t是树T的某个叶子节点，该叶子结点有$N_t$个样本,该叶子结点中k类的样本点有$N_{tk} 个， k = 1,2,3,…K$，$H_t(T)为叶节点t上的经验熵，$ a&gt;=0为参数，则决策树学习的损失函数可以定义为&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$C_a(T)=\\sum_{t=1}^{|T|} N_tH_t(T)+a|T|$ 这个T代表了树，不一定非得是跟，也可能是某个节点组成的小树，思维不要僵化。 这其实是一种添加正则化项的思想。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;其中，经验熵为$H_t(T)=-\\sum_{k}\\frac{N_{tk}}{N_t}log\\frac{N_tk}{N_t}$ &emsp;&emsp;&emsp;&emsp;叶节点t所有样本中k类的个数/叶节点t的总样本数&emsp;&emsp;在损失函数中，将上式的第一项记作：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)=-\\sum_{t=1}^{|T|}\\sum_{k=1}^{K}N_{tk}log\\frac{N_{tk}}{N_t}这时候有：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;C_a(T)=C(T)+a|T|&emsp;&emsp;&emsp;&emsp;$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数控制两者之间的影响，较大的a促使选择简单的树，较小的a选择复杂的树。a=0意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。&emsp;&emsp;&emsp;&emsp;剪枝就是当a确定时，选择损失函数最小的模型，即损失函数最小的子树。当a确定时，子树越大，往往拟合越好，但是模型复杂度就越高；子树越小则反之，损失函数正好表示了对两者的平衡。 输入：树T，参数a输出：剪枝后的T（1）计算每个结点的经验熵（2）递归的从树的叶子结点向上回缩。 &emsp;&emsp;&emsp;&emsp;设一组叶节点回缩到其父节点之前与之后整体树分别为$T_B和T_A$，其对应的损失函数值分别为$C_a(T_B)$和$C_a(T_A)$，如果$C_a(T_A)$&lt;=$C_a(T_B)$,说明这个非叶子节点没必要划分，则进行剪枝，因为划分反而会影响结果，就把该非叶子节点换成叶子节点，把它之前的儿子们中的样本全部放在现在这个节点中，取最多的类当做分类结果。（3）返回（2），直到不能继续为止，得到损失函数最小的子树$T_a$ 光说不练假把式，我们来一个例子试试。 例子下图是我们已经生成的一颗决策树， 由于算法顺序是从下往上、所以我们先考察最右下方的 Node（该 Node 的划分标准是“测试人员”），该 Node 所包含的数据集如下表所示： 颜色 测试人员 结果 黄色 成人 爆炸 黄色 小孩 不爆炸 我们现在开始第一次尝试剪枝：剪枝前：剪枝前该结点（测试人员的那个节点），有2个叶子结点，我们来计算他的损失为$C_a(T)=C(T)+a|T|$ C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)H_t(T)=-\\sum_{k}\\frac{N_{tk}}{N_t}log\\frac{N_tk}{N_t}我们经过带入后得到 C(T)=1*-1/1*log1/1 + 1*1/1*log1/1 = 0 注意这时候的T不是整个树，而是右下角那个测试人员的那棵小树。所以为$C_a(T) = |T|a= 2a$ 剪枝后：剪枝之后，之前这个节点是非叶子结点，现在变成了叶子结点，我们来计算一下损失 C(t)=2* (-\\frac12*log\\frac12 - \\frac12*log\\frac12) = 2所以 C_a(T) = |T|a= 2+a回忆生成算法的实现，我们彼时将α定义为了α=特征个数/2（注意：这只是α的一种朴素的定义方法，很难说它有什么合理性、只能说它从直观上有一定道理；如果想让模型表现更好、需要结合具体的问题来分析α应该取何值）。由于气球数据集 1.0 一共有四个特征、所以此时α=2；结合各个公式、我们发现： &emsp;&emsp;&emsp;&emsp; &emsp;&emsp;&emsp;&emsp;C_a(T)=2a=4=2+a=C_a(t) 所以我们已经进行局部剪枝，局部剪枝后的决策树如下图所示： 注意：进行局部剪枝后，由于该 Node 中样本只有两个、且一个样本类别为“不爆炸”一个为“爆炸”，所以给该 Node 标注为“不爆炸”、“爆炸”甚至以 50%的概率标注为“不爆炸”等做法都是合理的。为简洁，我们如上图中所做的一般、将其标注为“爆炸” 现在我们开始尝试第二次剪枝： 然后我们需要考察最左下方的 Node（该 Node 的划分标准也是“测试人员”），易知计算过程和上述的没有区别。对其进行局部剪枝后的决策树如下图所示： 现在我们开始尝试第三次剪枝：然后我们需要考察右下方的 Node（该 Node 的划分标准是“动作”），该 Node 所包含的数据集如下表所示： 颜色 测试人员 测试动作 结果 黄色 成人 用手打 爆炸 黄色 成人 用脚踩 爆炸 黄色 小孩 用手打 不爆炸 黄色 小孩 用脚踩 爆炸 紫色 成人 用脚踩 爆炸 紫色 小孩 用脚踩 爆炸 剪枝前：剪枝之前这个节点（动作）有2个叶子结点，一个叶子结点中有2个样本，一个叶子结点中有4个样本，让我们来计算一下这个结点的损失：&emsp;&emsp;&emsp;&emsp;C_a(T)=C(T)+a|T|=C(T)+2a 这个T代表动作这个小树&emsp;&emsp;&emsp;&emsp;C(T)=2*(-\\frac12*log\\frac12--\\frac12*log\\frac12)+4*(\\frac44*log\\frac44+0) 后面一项等于0，所以C(T)=2*(-\\frac12*log\\frac12--\\frac12*log\\frac12)=2 剪枝后：剪枝后改结点变成了叶子结点，有6个样本，我们来计算一下损失&emsp;&emsp;&emsp;&emsp;C_a(t)=C(t)+a|t|=C(t)+a&emsp;&emsp;&emsp;&emsp;C(T)=6*(-\\frac16*log\\frac16-\\frac56*log\\frac56) = 3.9将a=2带入后得剪枝后的损失&lt;剪枝前的损失，所以应该进行剪枝。 后面的计算类似 我就不说了。 记住是对每棵树的每个叶子结点计算经验熵。 写到这里的时候我才知道剪枝还分为预剪枝和后剪枝，我刚才说的是后剪枝的一种方法，其实后剪枝还有很多方法。我再介绍一种方法。 这种方法需要一个验证集来进行验证。你把这棵树的每个非叶子结点用验证集来对比剪枝前和剪枝后的结果，如果剪后的结果好于或者等于剪枝前，就可以进行剪枝，否则不进行剪枝，就是这样。不过这种方法不好的地方在于需要一个验证集。 我们再来介绍一下预剪枝，预剪枝的话方法比较少，这里我只介绍一种方法。 预剪枝就是在构造决策树的过程中，先对每个结点在划分前进行估计，若果当前结点的划分不能带来决策树模型泛华性能的提升，则不对当前结点进行划分并且将当前结点标记为叶结点。不过也需要一个验证集过程（1）先计算所有特征的信息增益比，找到一个最合适的特征来划分决策树。（2）但是因为是预剪枝，所以要判断是否应该进行这个划分，判断的标准就是看划分前后的泛华性能是否有提升，也就是如果划分后泛华性能有提升，则划分；否则，不划分。（3）如果遇到了某个叶子结点划分后不如不划分，则这个叶子节点就不划分了，改去划分别的结点。 给个例子：决策树用验证集来验证是否剪枝的例子 预剪枝总结：对比未剪枝的决策树和经过预剪枝的决策树可以看出：预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但是，另一方面，因为预剪枝是基于“贪心”的，所以，虽然当前划分不能提升泛华性能，但是基于该划分的后续划分却有可能导致性能提升，因此预剪枝决策树有可能带来欠拟合的风险。 后剪枝总结：对比预剪枝和后剪枝，能够发现，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[]},{"title":"决策树的整理","slug":"decision-tree","date":"2018-07-31T06:26:06.000Z","updated":"2018-08-01T01:51:17.485Z","comments":true,"path":"decision-tree/","link":"","permalink":"https://pspxiaochen.club/decision-tree/","excerpt":"","text":"什么是决策树&emsp;&emsp;分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有2种类型：内部节点和叶结点。内部节点表示一个特征或属性，叶结点表示一个类别。&emsp;&emsp;决策树是一种分类和回归的基本模型，可从三个角度来理解它，即：&emsp;&emsp;1.一棵树&emsp;&emsp;2.if-then规则的集合，该集合是决策树上的所有从根节点到叶节点的路径的集合&emsp;&emsp;3.定义在特征空间与类空间上的条件概率分布，决策树实际上是将特征空间划分成了互不相交的单元，每个从根到叶的路径对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。实际中，哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别。 决策树的优点1.可解释性强，容易向业务部门人员描述2.分类速度快 如何学习一棵决策树？决策树的学习本质上就是从训练数据集中归纳出一组分类规则，使它与训练数据矛盾较小的同时具有较强的泛华能力。从另一个角度看，学习也是基于训练数据集估计条件概率模型（至此，回答完了模型部分，下面接着说策略和算法）。决策树的损失函数通常是正则化的极大似然函数，学习的策略是以损失函数为目标函数的最小化（说完了策略，该说算法了）。 由于这个最小化问题是一个NP完全问题，现实中，我们通常采用启发式算法（这里，面试官可能会问什么是启发式算法，要有准备，SMO算法就是启发式算法）来近似求解这一最优化问题，得到的决策树是次最优的。 该启发式算法可分为三步： 特征选择模型生成决策树的剪枝 特征选择特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的，所以这种特征理论上是可以扔掉了。通常特征选择的准则是信息增益或信息增益比。 信息增益先给出熵和条件熵的定义：在概率统计中，熵表示随机变量的不确定性的度量。设X是一个取有限个值的离散随机变量，概率分布为&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P(X = x_i)=p_i,i=1,2,…,n$则随机变量X的熵定义为$H(X) = -\\sum_{i=1}^np_ilogp_i$由定义可知，熵只依赖于X的分布，而于X的取值无关，所以也可将X的熵记作$H(p)$$H(p) = -\\sum_{i=1}^np_ilogp_i$熵越大，随机变量的不确定性就越大。对同一个随机变量，当它的概率分布为均匀分布时，不确定性最大，熵也最大。对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大（能取的值越多说明不确定性就越大？）。（这是精华） 条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的H$H(Y|X)$,定义为X给定条件下Y的条件概率部分的熵对X的数学期望。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$H(Y|X) = \\sum_{i=1}^np_iH(Y|X=x_i)$这里，$p_i = P(X=x_i),i=1,2,…,n.$ 当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。 信息增益的定义特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D）与特征A给定条件下D的经验条件熵H(D|A)之差，$g(D,A)=H(D)-H(D|A)$一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。信息增益表示由于特征A而使得对数据集D的分类的不确定性减少的程度。H(D|A)小，证明A对D的分类不确定小，所以说A特征可以对D正确的分类，所以说信息增益越大越好。 设训练数据集为D，|D|表示其样本个数。设一共有K个类别$C_k$ , $|C_K|$表示属于类$C_k$的样本个数。设A有n个不同的取值{$a_1,a_2,…,a_n$},根据特征A的取值将D划分为n个子集{$D_1,D_2,…,D_n$},&emsp;&emsp;|$D_i$|为$D_i$的样本个数 记子集$D_i$中属于类$C_k$的样本集合$D_{ik}$,&emsp;&emsp;|$D_{ik}$|为$D_{ik}$的样本个数。 输入：训练数据集D和特征A：输出：特征A对训练数据集的信息增益g(D,A)先计算数据集D的熵H(D):$H(D) = -\\sum_{k=1}^K\\frac{|C_k|}{|D|}log\\frac{|C_k|}{|D|} $再计算条件熵H(D|a)$H(D|A) = \\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i) =-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log\\frac{|D_{ik}|}{|D_i|}$$g(D,A)=H(D)-H(D|A)$ 这种选择特征的思路就是ID3算法选择特征的核心思想。 信息增益比公式为：$g_R(D,A)=g(D,A)/IntI(D,A)$ 本来ID3算法计算信息增益好好的，但是C4.5一定要计算信息增益比这是为什么呢？比如我们有10个样本，样本中有1个特征是学号我们把它叫作A，我们知道每个学生的学号都是不一样的，这样我们在计算H(D|A)的时候发现这个值是0，这种情况会导致你会认为这个特征是非常的好的，其实学号这个特征用来切分样本并不是什么好的特征。 那么导致这样的偏差的原因是什么呢？从上面的例子应该能够感受出来，原因就是该特征可以选取的值过多。解决办法自然就想到了如何能够对树分支过多的情况进行惩罚，这样就引入了下面的公式，属性A的内部信息&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$IntI(D,A)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}log(\\frac{|D_i|}{|D|})$ 这样的话 如果还是对于学号这个特征会添加一个惩罚项10(-1/10)log(1/10),这个值一般是一个大于1的值。所以会起到惩罚的作用。 模型生成ID3算法ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归的构建决策树、输入：训练数据集D，特征集A，阈值o输出：决策树T（1）如果D中所有实例属于同一类$C_k$,则T为单节点树，并将类$C_k$作为该结点的类标记，返回T；（2）如果A=None，则T为单节点树，并将D中样本中类别最多的$C_k$作为该结点的类标记，返回T；（3）否则选择信息增益最大的特征$A_g$; (4) 如果$A_g$的信息增益小于阈值o，则说明所有的特征的信息增益都小于阈值，那么我们认为这些特征都很垃圾，那么我们就将D中样本类别最多的$C_k$作为该结点的类标记，返回T；（5）否则，对$A_g$的每一可能值$a_i$，根据$A_g=a_i$将D分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；（6）对每个子结点递归的调用，比如对第i个子结点，以$D_i$作为训练集，以A-${A_g}$作为可选特征集，调用（1）-（5），得到子树Ti,返回Ti. c4.5算法c4.5和ID3的唯一不同就是采用信息增益比来选择特征，其他全部一样。 一般递归的终止条件是什么：1.所有训练数据子集被基本正确分类2.没有合适的特征可选，即可用特征为0，或者可用特征的信息增益或信息增益比都很小了。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[]},{"title":"学习CNN时候的一些疑惑总结","slug":"cnn","date":"2018-07-21T03:21:59.000Z","updated":"2018-07-21T10:33:09.535Z","comments":true,"path":"cnn/","link":"","permalink":"https://pspxiaochen.club/cnn/","excerpt":"","text":"如何计算卷积层的输出尺寸：输入矩阵格式：四个维度，依次为：样本数，图像高度，图像宽度，图像通道数（RGB就是3）输出矩阵格式：四个维度，依次是，样本数，图像高度，图像宽度，图像通道数（后3个通道一般会发生变化）权重矩阵（卷积核）格式：同样是四个维度，依次是，卷积核的高度，卷积核的宽度，输入通道数，输出通道数（卷积核个数） 计算公式是：$height_{out} = (height_{in} - height_{kernel} + 2 padding) / 步长 + 1$$width_{out} = (width_{in} - width_{kernel} + 2 padding) / 步长 + 1$ 具体的细节可以参考链接CNN中卷积层的计算细节 如何计算池化层的输出尺寸：输入矩阵格式：四个维度，依次为：样本数，图像高度，图像宽度，图像通道数池化窗口的大小：一般取四个维度,依次是[1,height,width,1]因为我们不想在样本和图像通道上做池化，所以这两个维度设为了1。计算公式是：$height_{out} = (height_{in} - height_{filter}) / 步长 + 1$$width_{out} = (width_{in} - width_{filter}) / 步长 + 1$ 关于feature map的一些理解：比如有一个3232的RBG图像，这张图片在没有经过卷积之前有3张feature map，卷积核的大小是55，通道数是200，相当于是有200个卷积核，每个卷积核在原图像卷积得到一个featuremap,200个卷积核产生200个featuremap。 卷积层的作用：我认为卷积层作用就是提取特征，不同的滤波器有不同的权重，可以提取不同的特征，比如说颜色深浅，图像的轮廓。 池化层的作用： 不变性，更关注是否存在某些特征而不是特征具体的位置。可以看作加了一个很强的先验，让学到的特征要能容忍一些的变化。（包括平移，旋转，尺度。） 减小下一层输入大小，减小计算量和参数个数 获得定长输出。（文本分类的时候输入是不定长的，可以通过池化获得定长输出） 防止过拟合或有可能会带来欠拟合。 如何理解卷积神经网络中的权值共享？所谓的权值共享就是说，给一张图片，用一个滤波器去扫描，filter里面的参数就是权重，这张图的每个位置是被同样的filter扫的，所以权重是一样的，也就叫共享。 如何理解激活函数：因为线性模型的表达能力不够，引入激活函数是为了添加非线性因素。 怎么计算CNN的参数个数，连接数个数，复杂度：假设RGB图像的尺寸是32 32，滤波器的大小是 5 5，一共有200个滤波器，每个滤波器有1个bias参数，问有多少个训练参数，有多少连接，复杂度是多少？训练参数：滤波器553=75,加上一个bias参数，一共是76个，一共有200个滤波器，所有一共有76200=15200个参数。连接个数：经过卷积之后的输出图像大小为 2828，所以连接总数为：2828（553+1）*200 复杂度：时间复杂度即模型的运算次数。单个卷积层的时间复杂度：Time~O(M^2 K^2 Cin * Cout) 注1：为了简化表达式中的变量个数，这里统一假设输入和卷积核的形状都是正方形。注2：严格来讲每层应该还包含1个Bias参数，这里为了简洁就省略了。M:输出特征图（Feature Map）的尺寸。K:卷积核（Kernel）的尺寸。Cin:输入通道数。Cout:输出通道数。 全连接层的作用：全连接层之前的东西都是对提取的特征做各种处理，而到了全连接层我相当于对之前提取的特征做了一个全卷积的操作，比如我的全连接层输出是1000，那么这1000代表对1000种特征进行分类（有或者没有）最后在进行最终的分类。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[]},{"title":"红黑树的一些简单介绍","slug":"rb-tree","date":"2018-07-16T10:03:07.000Z","updated":"2018-07-17T09:40:05.548Z","comments":true,"path":"rb-tree/","link":"","permalink":"https://pspxiaochen.club/rb-tree/","excerpt":"","text":"红-黑树是基于二叉搜索树的，如果对二叉搜索树不了解，可以先看看：二叉搜索树 红黑树的主要规则：1.每个节点不是红色就是黑色。2.根节点一定是黑色的。3.如果一个节点是红色的，那么它的两个子节点都必须是黑色的。（反之不一定）4.从根节点到每个叶子节点或者空子节点的路径，都必须包含相同数目的黑色节点。 补充：红黑树没有AVL树那么平衡。它有它自己的平衡方法，满足了上面4条就叫平衡了。 如果添加或者删除节点之后打破了平衡，那么通过改变节点颜色，左旋，右旋可以使红黑树恢复平衡。 具体的看这个链接里的讲解：红黑树","raw":null,"content":null,"categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://pspxiaochen.club/categories/数据结构/"}],"tags":[]},{"title":"二叉搜索树的删除","slug":"search-tree","date":"2018-07-14T02:40:29.000Z","updated":"2018-07-14T05:35:36.591Z","comments":true,"path":"search-tree/","link":"","permalink":"https://pspxiaochen.club/search-tree/","excerpt":"","text":"三种情况：&emsp;&emsp;删除节点时二叉搜索树中最复杂的操作，但是删除节点在很多树的应用中又非常重要，所以详细研究并总结下特点。删除节点要从查找要删的节点开始入手，首先找到节点，这个要删除的节点可能有三种情况需要考虑：1.该节点是叶子结点，没有孩子结点了。2.该节点有一个孩子节点。3.该节点有两个孩子节点。 第一种最简单，第二种也还是比较简单的，第三种就相当复杂了。下面分析这三种删除情况： 第一种情况：&emsp;&emsp;要删除叶子节点，只需要改变该节点的父节点对应子字段的值即可，由指向要删除的节点改为null就可以了。垃圾回收器会自动回收叶节点，不需要自己手动删掉； 第二种情况：&emsp;&emsp;当要删除的节点有一个子节点时，这个将要删除的节点只有2个连接：连向父节点和连向它唯一的子节点。我们需要间断这些连接，把它的子节点直接连接到它的父节点上即可，如果被删除的节点是父节点的左子节点，那么我就把要删除节点的子节点连接到被删除节点父节点的左子节点就行。右子节点同理。 第三种情况：&emsp;&emsp;第三种情况是最复杂的。如果要删除有两个子节点的节点，就不能只用它的一个子节点代替它。因此需要考虑另一种方法，寻找它的中序后继来代替该节点。那么如何找后继节点呢？&emsp;&emsp;首先得找到要删除的节点的右子节点，它的关键字值一定比待删除节点的大。然后转到待删除节点右子节点的左子节点那里（如果有的话），然后到这个左子节点的左子节点，以此类推，顺着左子节点的路径一直向下找，这个路径上的最后一个左子节点就是待删除节点的后继。如果待删除节点的右子节点没有左子节点，那么这个右子节点本身就是后继。（后继节点就是比这个要删除节点第一个大的数）。&emsp;&emsp;找到后继节点我们就可以开始删除了。 第一种情况：后继节点是需要删除节点的右节点的左后代，这种情况要执行以下四个步骤：1.把后继的右子节点给后继节点的父节点的左孩子（leftChild)字段。2.把要删除节点的右节点给后继节点的右孩子(rightChild)字段。3.把待删除节点从它父节点的leftChild或rightChild字段删除，把这个字段置为后继；(如果删除的是左孩子字段就把左孩子字段设置成后继）。4.将后继的leftChild字段置为待删除节点的左子节点。 第二种情况：如果后继节点就是待删除节点的右子节点（这个后继节点肯定没有左孩子），这种情况比较简单，只需要把后继为根的子树移动到删除节点的位置即可。样例和源码看这里二叉搜索树","raw":null,"content":null,"categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://pspxiaochen.club/categories/数据结构/"}],"tags":[]},{"title":"关于B树的一些简单的总结","slug":"btree","date":"2018-07-09T01:16:51.000Z","updated":"2018-07-09T02:50:14.909Z","comments":true,"path":"btree/","link":"","permalink":"https://pspxiaochen.club/btree/","excerpt":"","text":"概念&emsp;&emsp;先说一下，B-树，B树，是一个东西。B树和普通的平衡二叉树不同的是B树属于多叉树又名平衡多路查找树（查找的路径不只有2条），数据库索引技术里大量使用了B树。 规则&emsp;&emsp;（1）树中的每个节点最多拥有m个子节点且m&gt;=2,空树除外（m阶代表一个树的节点最多有多少条查找路径，m阶=m路，当m=2则是平衡二叉树，当m=3时叫3阶B树）&emsp;&emsp;（2）除了根节点以外，每个节点的关键字数量大于等于 ceil(m/2)-1个 (ceil向上取整的函数)，小于等于m-1个，非根节点关键字必须大于等于2。（关键字在节点中）&emsp;&emsp;（3）所有叶子节点均在同一层，叶子节点除了包含了关键字和关键字记录的指针外也有只想其他子节点的指针，只不过其地址都为null.&emsp;&emsp;（4）如果一个非叶子节点有N个子节点，则该结点的关键字个数等于N-1。（有3个子节点说明有3条路径，可以理解成2个关键字把一条数轴分成了3段，这每一段代表一条路径）。&emsp;&emsp;（5）所有节点关键字是按递增次序排列，并遵循左小右大原则。 例子 查找流程 如上图我要从上图中找到E字母 （1）获取根节点的关键字进行比较，当前根节点关键字为M，E要小于M（26个字母顺序），所以往找到指向左边的子节点（二分法规则，左小右大，左边放小于当前节点值的子节点、右边放大于当前节点值的子节点）； （2）拿到关键字D和G，D&lt;E&lt;G 所以直接找到D和G中间的节点； （3）拿到E和F，因为E=E 所以直接返回关键字和指针信息（如果树结构里面没有包含所要查找的节点则返回null）； 插入规则（1）当前是要组成一个5路查找树，那么此时m=5,关键字数必须大于等于ceil（5/2）-1小于等于5-1（关键字数小于cei(5/2) -1就要进行节点合并，大于5-1就要进行节点拆分,非根节点关键字数&gt;=2）；（2）满足节点本身比左边节点大，比右边节点小的排序规则; 删除规则（1）当前是要组成一个5路查找树，那么此时m=5,关键字数必须大于等于cei(5/2)-1，小于等于5-1，非根节点关键字数大于2； （2）满足节点本身比左边节点大，比右边节点小的排序规则; （3）关键字数小于二时先从子节点取，取中间值往父节点放，子节点没有符合条件时就向向父节点取； 基本的东西就这些，如果还需要进一步的学习，可以看这篇文章从B树、B+树、B*树谈到R 树","raw":null,"content":null,"categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://pspxiaochen.club/categories/数据结构/"}],"tags":[]},{"title":"哈希表的一些总结","slug":"hash","date":"2018-07-07T04:27:00.000Z","updated":"2018-07-07T09:12:28.091Z","comments":true,"path":"hash/","link":"","permalink":"https://pspxiaochen.club/hash/","excerpt":"","text":"散列表&emsp;&emsp;理想的散列表数据结构只不过是一个包含有关键字的具有固定大小的数组。我们把表的大小记作Tablesize.（最好让表的大小是一个质数）每一个关键字被映射到从0到TableSize-1这个范围中的某个数，并且被放到适当的单元中。这个映射就叫做散列函数，理想情况下它应该运算简单并且应该保证任何两个不同的关键字映射到不同的单元。不过这是不可能的，因为单元的数目是有限的，而关键字实际上是用不完的。&emsp;&emsp;这就是散列的基本想法。剩下的问题则是要选择一个函数，决定当关键字散列到同一个值的时候（称为冲突）应该做什么。 散列冲突&emsp;&emsp;如果当一个元素被插入时另一个元素已经存在（散列值相同），那么就产生一个冲突，这个冲突需要消除。我讲介绍其中最简单的2种方法：分离链接法和开放地址法。 分离链接法&emsp;&emsp;解决冲突的第一种方法通常叫做分离链接法，其做法是将散列到同一个值的所有元素保留到一个表中。为了方法起见，这些表都有表头。也就是数组里存了一个链表，为了以后删除元素方便，每个链表都有一个表头。如下图所示： &emsp;&emsp;分离链接法的缺点是需要指针，由于给新单元分配地址需要时间，一次这就导致算法的速度多少有些慢，同时算法实际上还要求对另一种数据结构的实现。 开放定址法&emsp;&emsp;开放定址散列法是另外一种不同链表解决冲突的方法。在这个方法中，如果有冲突发生，那么就要尝试选择另外的单元，直到找出空的单元为止。更一般的，单元$h_0(X),h_1(X),h_2(X)$等等，相继被试选，其中$h_i(X)=(Hash(X)+F(i))$ mod TableSize 函数F是冲突解决方法。 一般来说，对开放定址散列算法来说，装填因子应该低于$\\lambda$=0.5。接下来介绍3个通常解决冲突的方法。 线性探测法&emsp;&emsp;在线性探测法中，函数F是i的线性函数，典型情形是F(i)=i。这相当于逐个探测每个单元（必要时可以绕回）以查找出一个空单元。 如果表可以有多于一半被填满的话，那么线性探测就不是一个好办法。 平方探测法&emsp;&emsp;平方探测法是消除线性探测中一次聚集问题的冲突解决方法。平方探测就是冲突函数为二次函数的探测方法。流行的选择是$F(i)=i^2$&emsp;&emsp;对于线性探测，让元素几乎填满散列表并不是一个好的主意，因此测试表的性能会降低。对于平方探测情况甚至更糟：一旦表被填满超过一半，当表的大小不是质数时甚至在表被填满一半之前，就不能保证一次找到一个空单元了。这是因为最多有表的一半可以用作解决冲突的备选位置。&emsp;&emsp;对于这种情况，我们可以使用再散列,就是建立另外一个大约两倍大的表（并且使用一个相关的新散列函数），扫描这个那个原始散列表，计算每个（未删除的）元素的新散列值并将其插入到新表。&emsp;&emsp;在散列可以用平方探测以多种方法实现。一种做法是只要表满到一半就再散列。另一种极端的方法是只有当插入失败时才再散列。第三种是当表到达某一个装填因子时进行再散列。由于随着装填因子的增加表的性能的确有下降，因此第三种可能是最好的策略。 双散列&emsp;&emsp;对于双散列，一种流行的选择是$F(i)=i * hash(x)$.这个方法如果hash函数选择的不好将会是灾难性的。比如X=99，hash(x) = x mod 9.因此，函数一定不要算得0.","raw":null,"content":null,"categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://pspxiaochen.club/categories/数据结构/"}],"tags":[]},{"title":"这是一个天大的秘密","slug":"test","date":"2018-07-05T04:27:00.000Z","updated":"2018-07-05T09:52:39.552Z","comments":true,"path":"test/","link":"","permalink":"https://pspxiaochen.club/test/","excerpt":"","text":"其实什么都没有，这只是一个测试而已。","raw":null,"content":null,"categories":[{"name":"life","slug":"life","permalink":"https://pspxiaochen.club/categories/life/"}],"tags":[{"name":"秘密","slug":"秘密","permalink":"https://pspxiaochen.club/tags/秘密/"}]},{"title":"线性回归与最小二乘法","slug":"costfunction","date":"2018-07-03T04:27:00.000Z","updated":"2018-07-05T10:20:04.207Z","comments":true,"path":"costfunction/","link":"","permalink":"https://pspxiaochen.club/costfunction/","excerpt":"","text":"&emsp;&emsp;我们假设正确的结果y和我们的预测的输出函数有如下关系： &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$y^{(i)} = \\theta^Tx^{(i)} + \\epsilon^{(i)}$ 在这里$\\theta^Tx^{(i)}$为我们的预测函数，$\\epsilon^{(i)}$是和真实值的误差。因为每个样本都是独立的，因此误差直接也是独立的。所以我们假设$\\epsilon^{(i)}$服从期望是0（我们希望没有误差）,方差是$\\sigma^2$的高斯分布，记作$\\epsilon^{(i)}\\sim N(0,\\sigma^2)$,而高斯分布的概率密度函数为： &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$&emsp;&emsp;将误差带入上面的式子得：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P(\\epsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2})$ 因为我们假设$\\epsilon^{(i)}$服从期望是0（我们希望没有误差）,方差是$\\sigma^2$的高斯分布。所以还可以假设$y^{(i)}$是服从期望是里$\\theta^Tx^{(i)}$，方差为$\\sigma^2$的高斯分布，在给定 x(i)且参数为 θ的情况下，记作：$y^{(i)}\\sim N(\\theta^Tx^{(i)},\\sigma^2)$,所以我们可以将上面的式子改写为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$ 因为在我们的样本中，y(i) 已经给定了，我们需要找到一个参数 θ，使得我们最有可能去得到 y(i)的分布。我们想要估计其中的未知参数θ。由此我们可以想到一个非常常用的参数估计方法—极大似然估计。关于极大似然估计我推荐知乎的2篇文章，讲的浅显易懂。似然函数与极大似然估计一文搞懂极大似然估计 接着刚才，我们使用极大似然估计后可写成：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L(\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta) $ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L(\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$ 因为是极大似然估计，所以我们希望$L(\\theta)$要尽可能的大。所以我们对上面的式子取对数，因为对数不改变函数的单调性。 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L(\\theta)=\\sum_{i=1}^m log\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$=mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}+\\sum_{i=1}^m -\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$=mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\sum_{i=1}^m \\frac{1}{2\\sigma^2}*(y^{(i)}-\\theta^Tx^{(i)})^2$ 为了使$L(\\theta)$要尽可能的大，我们就需要让$J(\\theta)=\\frac{1}{2}*(y^{(i)}-\\theta^Tx^{(i)})^2$尽量的小，所以就有了平方损失函数，可以看到是一模一样的。J(θ) 即为此线性回归的cost function。由此我们可以非常自然地推导出为什么线性回归中的cost function是使用最小二乘法。接下来就是求解过程，常用的就是梯度下降，如果想知道为什么用梯度下降，请看这篇我自己理解的梯度下降原理","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"线性回归","slug":"线性回归","permalink":"https://pspxiaochen.club/tags/线性回归/"}]},{"title":"25周岁生日","slug":"birthday","date":"2018-06-29T11:27:00.000Z","updated":"2018-06-30T04:31:13.138Z","comments":true,"path":"birthday/","link":"","permalink":"https://pspxiaochen.club/birthday/","excerpt":"","text":"今天是25岁生日，祝我生日快乐，下午接了一个团购鞋子，真是讽刺。","raw":null,"content":null,"categories":[{"name":"life","slug":"life","permalink":"https://pspxiaochen.club/categories/life/"}],"tags":[]},{"title":"机器学习之交叉验证","slug":"cross-validation","date":"2018-06-29T02:27:00.000Z","updated":"2018-06-29T03:32:36.146Z","comments":true,"path":"cross-validation/","link":"","permalink":"https://pspxiaochen.club/cross-validation/","excerpt":"","text":"交叉验证的基本思想&emsp;&emsp; 交叉验证的基本想法是重复的使用数据；把给定数据进行切分，将切分的数据集组合为训练集与测试集，再次基础上反复地进行训练、测试、以及模型选择。 简单的交叉验证1、首先随机得将已给数据分为两部分，一部分作为训练集，一部分作为测试集。（一般是73分）2、然后用训练集在各种条件下（比如不同的参数）训练模型，从而得到不同的模型；3、在测试集上评价各个模型的测试误差，选出测试误差最小的模型。优点：由于测试集和训练集是分开的，就避免了过拟合现象 k折交叉验证1.首先将训练数据平均切分成k份，每一份互不相交且大小一样。2.用k-1个子集进行训练模型，用余下的那一个作为预测。3.将2这一过程对可能的k种选择重复进行。4.最后选出k次测评中测试误差最小的。、优点：这个方法充分利用了所有样本。但计算比较繁琐，需要训练k次，测试k次。 留一法留一法就是每次只留下一个样本做测试集，其它样本做训练集，如果有k个样本，则需要训练k次，测试k次。优点：留一发计算最繁琐，但样本利用率最高。适合于小样本的情况。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"交叉验证","slug":"交叉验证","permalink":"https://pspxiaochen.club/tags/交叉验证/"}]},{"title":"求余和取模傻傻分不清","slug":"rem-and-mod","date":"2018-06-27T13:27:00.000Z","updated":"2018-06-27T12:21:38.761Z","comments":true,"path":"rem-and-mod/","link":"","permalink":"https://pspxiaochen.club/rem-and-mod/","excerpt":"","text":"&emsp;&emsp;通常情况下取模运算(mod)和求余(rem)运算被混为一谈，因为在大多数的编程语言里，都用’%’符号表示取模或者求余运算。在这里要提醒大家要十分注意当前环境下’%’运算符的具体意义，因为在有负数存在的情况下，两者的结果是不一样的。&emsp;&emsp;假设有整数a和b，取模或者求余运算的方法都是（1）c=a/b (2)r=a-c*b&emsp;&emsp;求模运算和求余运算在第一步不同,取余运算在计算商值向0方向舍弃小数位,取模运算在计算商值向负无穷方向舍弃小数位.&emsp;&emsp;在C中 %是取余，mod是取模。&emsp;&emsp;在Python中%就是取模。 例子：如果a=3 b=-43/(-4)等于-0.75在取余运算时候商值向0方向舍弃小数位为0在取模运算时商值向负无穷方向舍弃小数位为-1所以3rem(-4) = 33mod(-4) = -1希望这次可以记住。","raw":null,"content":null,"categories":[{"name":"笔试","slug":"笔试","permalink":"https://pspxiaochen.club/categories/笔试/"}],"tags":[{"name":"mod","slug":"mod","permalink":"https://pspxiaochen.club/tags/mod/"}]},{"title":"kd树-第一次面试的痛","slug":"kd-tree","date":"2018-06-26T13:27:00.000Z","updated":"2018-06-27T02:54:08.510Z","comments":true,"path":"kd-tree/","link":"","permalink":"https://pspxiaochen.club/kd-tree/","excerpt":"","text":"什么是KD树&emsp;&emsp;kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分。构造kd树相当于不断的用垂直于坐标轴的超平面将K维空间切分，构成一系列的k维超矩形区域。kd树的每个节点对应于一个K维超矩形区域。 构建KD树输入：K维空间数据集，样本数为N。$x_i = (x_i^{(1)},x_i^{(2)},x_i^{(3)}….x_i^{(k)})^T$输出：KD树&emsp;&emsp;(1)开始：构造根节点，根节点包含了K维空间的超矩形区域。&emsp;&emsp;选择$x^{(1)}$作为第一个切分的维度，找到$x^{(1)}$坐标的中位数为切分点，将数据集一分为二，大于$x_i^{(1)}$的中位数在右，小于在左。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。&emsp;&emsp;将落在切分超平面的实例点保存在根节点。&emsp;&emsp;(2)重复切分操作，再将$x^{(2)}$作为切分的维度，并在之前已经一分为二的左边和右边分别找到$x^{(2)}$维度的中位数，接着分别切分到左右两边。&emsp;&emsp;(3)一直重复切分操作,对深度为j的节点,选择$x^{(l)}$为切分的坐标轴，l=j(mod)k +1,以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超平面矩形区域切分为两个子区域。将落在切分超平面的实例点保存在根节点。&emsp;&emsp;(4)直到两个子区域没有实例存在时停止。从而形成kd数的区域划分。 搜索kd树（最近邻）&emsp;&emsp;下面介绍如何利用kd树进行最近邻搜索。利用kd树可以省去大部分数据点的搜索，从而减少搜索的计算量。输入：（1）已经构造好的kd树（2）目标点x（想找到距离x最近的点）输出： x的最近邻&emsp;&emsp;（1）在kd树种找出包含目标点x的叶节点：从根节点出发，递归的向下访问kd树。若目标点x当前维（用哪个维度分的就用哪个维度）的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点。直到找到子节点为叶节点为止。&emsp;&emsp;（2）以次叶节点为‘当前最近点’&emsp;&emsp;（3）递归（刚才一路下来的那些点都要进行接来下来的操作）地向上回退，在每个结点都进行以下操作：&emsp;&emsp;&emsp;&emsp;a:如果该结点保存的实例点比当前最近点的距离（用选定的公式计算比如欧氏距离）目标点更近，则以该实例点为‘当前最近点’。&emsp;&emsp;&emsp;&emsp;b:检查目前结点的另一子结点对应的区域是否以目标点为球心、以目标点与‘当前最近点’间的距离为半径的超球体相交。&emsp;&emsp;&emsp;&emsp;若相交：可能在另一个子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点，接着递归的向下搜索&emsp;&emsp;&emsp;&emsp;如果不想交：向上回退。&emsp;&emsp;（4）当退回到根节点时，搜索结束。最后的‘当前最近点’即为x的最近邻点。 &emsp;&emsp;kd数搜索的评论计算复杂度是O(logN),N是训练实例数.kd树更适用于训练实例数远大于空间维数时的k近邻搜索。 搜索kd树*（K近邻）输入：给定一个已经构建好的kd树，需要寻找的点P，需要寻找几个近邻K。输出：L列表，L列表里有K个空位，每个空位装的是近邻点。 &emsp;&emsp;（1）根据切分的维度对比同维度下P的坐标值向下搜索。（也就是说，如果树的节点是照 $x_r=a$ 进行切分，并且 p 的 r 坐标小于 a，则向左枝进行搜索；反之则走右枝）。&emsp;&emsp;（2）当到达叶子结点时，将其标记为访问过。如果L列表中不足k个点，则将当前结点的坐标加入到L列表。如果L列表不为空并且当前结点与P的距离小于L中所存放结点的最大距离，则用当前结点替换掉L中离P最远的结点。&emsp;&emsp;（3）如果当前的结点不是整棵树最顶端的结点，执行a操作；反之，输出L列表。&emsp;&emsp;&emsp;&emsp;a:向上爬一个结点。如果当前（向上爬过之后的）结点没有被访问过，将其标记为访问过，然后执行b和c操作；如果当前结点被访问过，再次执行a.&emsp;&emsp;&emsp;&emsp;b:如果此时L列表中不足k个点，则将结点加入到L；如果L中已经满k个点，并且当前结点与P的距离小于 L列表中最远的距离的点，则用结点替换掉L中距离P最远的点。&emsp;&emsp;&emsp;&emsp;c:计算p和当前结点切分线的距离（作垂线）。如果该距离大于等于L列表中距离最远的点并且L列表中已经有k个点，则在当前结点的另外一边（没访问过的那边）不会有更近的点，接着执行（3）；如果该距离小于L列表中距离最远的点或者L列表中不足k个点，则切分线另一边可能有更近的点，因此在当前结点的另外一边可能有更近的点，因此在当前结点的另外一边从（1）开始重新执行。 大概过成就是这样，下面有一个链接，链接里有距离寻找K近邻的例子。搜索kd数k近邻搜索样例","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"kd树","slug":"kd树","permalink":"https://pspxiaochen.club/tags/kd树/"}]},{"title":"该死的牛客网输入。","slug":"code_input","date":"2018-06-25T13:27:00.000Z","updated":"2018-06-26T15:57:20.237Z","comments":true,"path":"code_input/","link":"","permalink":"https://pspxiaochen.club/code_input/","excerpt":"","text":"Python各种输入的方法。例一&emsp;&emsp;输入：1247 15 9 5 可以看到 第一行的输入的数是第二行输入数字的个数。1234import sysline = sys.stdin.readline()n = int(line) #读进来的都是str类型，需要转换成int类型nums = [int(t) for t in sys.stdin.readline().split()] 在这里我们主要看一下第4行代码12345678nums = [int(t) for t in sys.stdin.readline().split()]#这一行的输出是[7,15,9,5]#我们现在把他拆解一下，如果写成这样nums = sys.stdin.readline().split()#现在这一行的输出变成了['7','15','9','5'],和之前的区别是现在list里每一个元素都是一个字符。#如果再改成这样nums = sys.stdin.readline()#现在的输出就变成了7 15 9 5，注意这是一个str，比如nums[3]，就会输出5，当然5也是一个字符。 例二输入：123454 43332323333322323 可以看到第一行的第一个4控制接下来输入的行数，第一行的第二个4控制接下来每一行的输入的元素。1234567import sysm, n = [int(x) for x in sys.stdin.readline().split()]nums = [[int(x) for x in sys.stdin.readline().strip()] for i in range(m)]#重点看一下第3行的代码,相当于用了2层for循环#外循环是运行m次int(x) for x in sys.stdin.readline().strip()，内循环是将每一行输入进来的str转换成int类型#这一行将来的输出是一个二维数组[[3, 3, 3, 2], [3, 2, 3, 3],[3, 3, 3, 2],[2, 3, 2, 3]] 目前还没看到别的不同的例子，如果今后碰到再补充。","raw":null,"content":null,"categories":[{"name":"code","slug":"code","permalink":"https://pspxiaochen.club/categories/code/"}],"tags":[{"name":"笔试","slug":"笔试","permalink":"https://pspxiaochen.club/tags/笔试/"}]},{"title":"K近邻法(KNN)之不再遗忘。","slug":"KNN","date":"2018-06-22T13:27:00.000Z","updated":"2018-06-23T01:52:15.078Z","comments":true,"path":"KNN/","link":"","permalink":"https://pspxiaochen.club/KNN/","excerpt":"","text":"K近邻算法的基本&emsp;&emsp; k近邻法(k-nearest neighbor,KNN)是一种可以做分类也可以做回归的方法。是一种比较简单的方法，给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最近的K个实例，这K个实例的多数属于某个类，就把该输入实例分到这个类。 基本过程：&emsp;&emsp; 输入：训练数据集T和实例特征向量x.&emsp;&emsp; 输出：实例x所属的类y.&emsp;&emsp; 过程：（1）根据给定的距离度量，在训练集T中找到与x最近的k个点，涵盖着k个点的集合记作N。&emsp;&emsp;&emsp;&emsp; &emsp;（2）在N中根据分类决策规则决定x的类别y,比如可以将N中类别最多的类别就定义为x的类别。 特殊情况：&emsp;&emsp; 当k = 1的情形，称为最近邻算法。对于输入的实例点x，他的分类结果就是距离他最近的训练数据中的点。&emsp;&emsp; 可以看出k近邻算法没有显式的学习过程。 K近邻模型的三个基本要素三个基本要素分别是：距离度量、K值的选择、分类决策规则。 距离度量：&emsp;&emsp; 特征空间中两个实例点的距离是两个实例点相似程度的反应。&emsp;&emsp; 若有两个实例$x_i$和$x_j$，这两个实例的$L_p$距离被定义为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L_p(x_i,x_j)=(\\sum_{i=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$&emsp;&emsp; p是&gt;=1的，当p=2时,我们称为欧式距离：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L_p(x_i,x_j)=(\\sum_{i=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\\frac{1}{2}}$&emsp;&emsp;当p=1时，我们称为曼哈顿距离&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$L_p(x_i,x_j)=(\\sum_{i=1}^n|x_i^{(l)}-x_j^{(l)}|)$&emsp;&emsp;怎么通俗的理解曼哈顿距离和欧式距离呢&emsp;&emsp;假如你现在想从宿舍去食堂吃饭，你有2种走法第一种就是两点之间线段最近，直接走一条直线，管他有没有障碍物全部都挡不住你，你是电，你是光，你是唯一的神话，这就叫欧氏距离。&emsp;&emsp;还有一种走法就是你肯定得按照学校的规划道路上走，必须在路上，你不能穿墙，上天，入地。所以你可能会有好几种走法，但是都必须在路上走。这种就叫做曼哈顿距离。还有很多种距离，我只介绍这两种。&emsp;&emsp;距离度量的选择可能会影响之后的结果。 K值的选择：K值的选择会对K近邻法的结果产生重大的影响。&emsp;&emsp;先了解一下什么叫近似误差和估计误差&emsp;&emsp;近似误差：可以理解为对现有训练集的训练误差。&emsp;&emsp;估计误差：可以理解为对测试集的测试误差。 &emsp;&emsp;近似误差关注训练集，如果近似误差小了会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。模型本身不是最接近最佳模型。&emsp;&emsp;如果选择较小的k值，就相当于只有与输入实例较近（相似）的实例才会对预测结果起作用，‘学习’的近似误差会减小，估计误差会增大，如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，K值的减小意味着整体模型变的复杂，容易发生过拟合。 &emsp;&emsp;如果选择比较大的k值，就相当于用较多的训练实例进行预测。有点是可以减少学习的估计误差，缺点是学习的近似误差会增大。这时与输入实例较远（不相似）的训练实例也会对预测起作用，使预测发生错误。K值的增大就意味着整体的模型变得简单。&emsp;&emsp;如果K = N(样本总量) ，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。&emsp;&emsp;在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。 分类决策规则&emsp;&emsp;K近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。 KNN的优缺点优点1.简单，易于理解，易于实现，无需参数估计，无需训练，既可以用来做分类也可以用来做回归；2.可用于数值型数据和离散型数据；3.训练时间复杂度为O(n)；无数据输入假定；4.对异常值不敏感,kNN不会受到差别特别大的样本中的特征元素的影响(对异常值不敏感)。因为采用了归一化技术。 缺点：1.计算复杂性高；空间复杂性高；2.可解释性差，无法告诉你哪个变量更重要，无法给出决策树那样的规则；3.K值的选择：最大的缺点是当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进;4.最大的缺点是无法给出数据的内在含义。5.消极学习方法、懒惰算法。 伪代码 计算已知类别数据集中的点与当前点之间的距离（值域越大的变量常常会在距离计算中占据主导作用，需要进行归一化或者标准化处理）； 按照距离递增次序排序； 选择与当前距离最小的k个点； 确定前k个点所在类别的出现概率 ； 返回前k个点出现频率最高的类别作为当前点的预测分类。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"https://pspxiaochen.club/tags/KNN/"}]},{"title":"朴素贝叶斯之了断心结。","slug":"bys","date":"2018-05-30T13:27:00.000Z","updated":"2018-06-25T03:37:41.161Z","comments":true,"path":"bys/","link":"","permalink":"https://pspxiaochen.club/bys/","excerpt":"","text":"&emsp;&emsp; 为什么说贝叶斯一直是我的心结呢。这说来就话长了，当初考研时，想都没想就选择了相对简单的专硕（现在看来其实也并不简单），而我们学习的专硕是不考概率论的，所以基本上我的概率论体系还停留在高中阶段。那你又要问了，难道一个计算机科班出身的，本科就没学过概率论么，说到这里我就想起来我的概率论老师上课操着一口湖南口音，声音很小，150个人的大教室，而我在最后一排和舍友疯狂的搓着3DS，咳咳。说远了。今天我一定用最通俗理解的方法把朴素贝叶斯里的每一个知识点都梳理一遍，算是对自己有一个交代。 如果没什么概率论基础的话先推荐一篇文章应该如何理解概率分布函数和概率密度函数？ 生成式模型和判别式模型。&emsp;&emsp;在讲贝叶斯之前我觉得我有理由先说一下什么是生成式模型而什么是判别式模型。在《统计学习方法》书中明确写道：监督学习方法又可以分为生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。 生成式模型：&emsp;&emsp;生成方法由数据学习联合概率分布P(X,Y),然后求出条件概率分布P（Y|X）作为预测模型，即生成模型 P（Y|X）= P(X,Y) / P(X),这样的方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。我们今天的主角朴素贝叶斯就是生成模型，常见的还有隐马尔科夫模型。&emsp;&emsp;生成方法的特点：生成方法可以还原出联合概率分布P（X,Y），而判别方法则不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型。 判别式模型：&emsp;&emsp;判别方法由数据直接学习决策函数f(X)或者条件概率分布P（Y|X）作为预测的模型。判别方法关系的是对给定的输入X，应该预测什么样的输入出Y。&emsp;&emsp;判别方法的特点：判别方法直接学习的是条件概率P（Y|X）或者决策函数F（X），直接面对预测，往往学习的准确率更高。由于直接学习P（Y|X）或者F（X），可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 总结&emsp;&emsp;两种模型都是利用条件概率判别，只不过生成模型通过求解联合分布得到条件概率，而判别模型直接计算条件概率，如果你不知道什么是联合概率分布，我一会会解释，别怕。 现在开始我们的主菜（朴素贝叶斯）&emsp;&emsp;首先先声明一点，朴素贝叶斯与贝叶斯估计是不同的概念。那为什么叫朴素呢。是因为他假设特征条件都相互独立（不过一般都是不可能的）。所以说朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。&emsp;&emsp;X是定义在输入空间上的随机向量，Y是定义在输出空间上的随机变量。P（X,Y）是X和Y的联合概率分布。训练数据集由P（X,Y）独立同分布产生。&emsp;&emsp;朴素贝叶斯通过训练数据集学习联合概率分布P（X,Y），那我们现在说说什么是联合概率分布。 联合概率分布&emsp;&emsp;联合概率分布简称联合分布，是两个及以上随机变量组成的随机向量的概率分布。根据随机变量的不同，联合概率分布的表示形式也不同。对于离散型随机变量，联合概率分布可以以列表的形式表示，也可以以函数的形式表示；对于连续型随机变量，联合概率分布通过一非负函数的积分表示。打靶时命中的坐标（x，y）的概率分布就是联合概率分布（涉及两个随机变量），其他同样类比。 连续型联合概率分布&emsp;&emsp;对于二维连续随机向量，设X,Y为连续性随机变量，其联合概率分布或连续性随机变量（X,Y）的概率分布F（x,y）通过一非负函数f(x,y) &gt;= 0的积分表示，称函数f(x,y)为联合概率密度函数。两者关系如下：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 在接上面，我们想要通过训练数据集学习联合概率分布P（X,Y）= P(XY），而我们知道条件概率公式： P(AB)=P(A|B)P(B)=P(B|A)P(A) ，所以我们只要知道先验概率分布以及条件概率分布。就可以得到联合概率分布。&emsp;&emsp;朴素贝叶斯对条件概率分布做了条件独立性的假设。这个是较强的假设。具体的，条件独立性假设是：&emsp;&emsp;&emsp;&emsp;（1）这个公式可以这么理解：当事件A1，A2，A3相互独立时，有P(A1,A2,A3）= P（A1)P(A2)P(A3)p(A1,A2,A3|B) = P(A1|B)P(A2|B)P(A3|B) &emsp;&emsp;朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型计算后验概率分布P=（Y=c_k|X=x)，将后验概率最大的类作为x的类输出，后验概率计算根据贝叶斯定理进行,现在开始手推公式：由条件概率公式得到：$P(Y=c_k|X=x)=\\frac{p(X=x,Y=c_k)}{P(x)} = \\frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}$ （2） 再由全概率公式得到：$P(X=x) = \\sum_{k} P(Y=c_k)P(X=x|Y=c_k)$ （3） 将（3）带入（2）得到：$P(Y=c_k|X=x)=\\frac{p(X=x,Y=c_k)P(Y=c_k)}{\\sum_{k} P(Y=c_k)P(X=x|Y=c_k)}$ （4） 再将（1）带入（4）得到：$P(Y=c_k|X=x)=\\frac{P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}，k=1,2,3,K。$ 到此朴素贝叶斯分类器可以表示为：$y=f(x)=\\arg\\min_{c_k}\\frac{P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}$ 由于分母就是$P(X=x)$ 所以对于所有c_k来说都是相等的，所以可以把分母省略掉。所以最后的公式为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$y=f(x)=\\arg\\min_{c_k}{P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|P(Y=c_k)}$ 后验概率最大化的含义朴素贝叶斯将实例分到后验概率最大的类中，也就是希望损失函数尽量的小，损失函数越小，模型越好，只需所有的样本点都求一次损失函数然后进行累加就是经验风险，我们希望让经验风险也尽可能的越小越好，经验风险是对训练集中的所有样本点损失函数的平均最小化。经验风险越小说明模型f(X)对训练集的拟合程度越好，但是对于未知的样本效果怎么样呢？我们知道未知的样本数据（X,Y）的数量是不容易确定的，所以就没有办法用所有样本损失函数的平均值的最小化这个方法，那么怎么来衡量这个模型对所有的样本（包含未知的样本和已知的训练样本）预测能力呢？熟悉概率论的很容易就想到了用期望。这下我们又得再来说说数学期望了，数学期望想必都比较数学，我只说一下离散型和连续型求法的区别。 离散型：$E(X)=\\sum_{i}x_ip_i$ 连续型：$E(X)=\\int_{-\\infty}^{+\\infty} {xf(x)} \\,{\\rm d}x$&emsp;&emsp;&emsp;&emsp;&emsp;$f(x)$为概率密度函数。 假设我们的损失函数是0-1损失函数：&emsp;&emsp;&emsp;&emsp;设X和Y服从联合分布P(X,Y).那么期望风险就可以表示为：$R_{exp}(f) = E[L(Y,f(x))]$接着推公式得到：$R_{exp}(f) = E[L(Y,f(x))]$可以看出最后变成了条件期望。&emsp;&emsp;这就是期望风险，期望风险表示的是全局的概念，表示的是决策函数对所有的样本预测能力的大小，而经验风险则是局部的概念，仅仅表示决策函数对训练数据集里样本的预测能力。理想的模型（决策）函数应该是让所有的样本的损失函数最小的（也即期望风险最小化），但是期望风险函数往往是不可得到的，即上式中，X与Y的联合分布函数不容易得到。现在我们已经清楚了期望风险是全局的，理想情况下应该是让期望风险最小化，但是呢，期望风险函数又不是那么容易得到的。怎么办呢？那就用局部最优的代替全局最优这个思想吧。这就是经验风险最小化的理论基础。&emsp;&emsp;为了使条件期望最小化，只需要对X=x每一个都极小化： 根据期望风险最小化准则就得到了后验概率最大化准则。 总的来说 朴素贝叶斯就是把朴素的思想（条件独立性）带入到贝叶斯公式之中，来得到朴素贝叶斯分类器。中间需要计算先验概率和条件概率，然后通过极大似然估计可以估计这些参数。 最后在推荐2篇文章，里面写的十分详细。数学之美番外篇：平凡而又神奇的贝叶斯方法贝叶斯推断及其互联网应用（一）：定理简介","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"贝叶斯","slug":"贝叶斯","permalink":"https://pspxiaochen.club/tags/贝叶斯/"}]},{"title":"我自己理解的梯度下降的原理","slug":"2018-05-24-GD","date":"2018-05-23T13:27:00.000Z","updated":"2018-07-05T09:38:04.650Z","comments":true,"path":"2018-05-24-GD/","link":"","permalink":"https://pspxiaochen.club/2018-05-24-GD/","excerpt":"","text":"&emsp;&emsp;最近不太顺啊，各种碰壁，看来学习确实容不得半点虚假，会就是会，不会就是不会。现在决定再认认真真复习一遍机器学习和数据结构，再来巩固一下自己的知识体系，只希望这次可以别忘的太快。 我们为什么要使用梯度下降。&emsp;&emsp;因为机器学习总的来说是一个优化问题。我们有一个想要优化的函数，比如说损失函数，我们总是希望可以让损失函数变得越来越小。那怎么才能使损失函数变得原来越小呢？损失函数中有一些未知的参数，我们只要不停更新这个参数，就可以让损失函数越来越小。直到损失函数等于0或者不再发生变化为止。所以说梯度下降是用来更新一些未知参数的，而这些参数会使损失函数越来越小。 梯度下降是怎么来的呢？&emsp;&emsp;首先我们来看一元函数的泰勒展开，以便于更好的理解多元函数的泰勒展开。如果一个一元函数n阶可导，它的泰勒展开公式为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;如果在某一点处的导数值大于0（+），则函数在此处是增函数，加大x的值函数会增加，减小x的值函数会减小。相反的，如果在某一点处导数值小于0（-），则函数是减函数，增加x的值函数值会减小（+），减小x的值函数会增加。因此我们可以得出一个结论：如果x的变化很小，并且变化值与导数值反号，则函数值下降。对于一元函数，x的变化只有两个方向，要么朝左，要么朝右。 &emsp;&emsp;下面我们把这一结论推广到多元函数的情况。多元函数f(x)在x点处的泰勒展开为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;这里我们忽略了二次以及更高的项。其中一次项是梯度向量▽f(x)与自变量的增量△X的內积，这等价于一元函数的一次项f’(X0)(X-X0)。这样，函数的增量与自变量的增量△X、函数梯度的关系可以表示为：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;如果△X足够小，在X的某一领域内，则我们可以忽略二次及以上的项，有：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;这里的情况比较复杂，△X是一个向量，有无穷多种方向，该往哪个方向走呢？如果能保证上面约等式的右边恒小于0，则有：f(x+△x) &lt; f(x)，即函数值递减，这就是下山的正确方向。如果这句话还不能理解，我们可以再换一种方式理解。假如x是我们想要更新的参数,f(x)是我们刚开始需要优化的损失函数,我们很希望可以让f(x)越来越小，那怎么才能够让其越来越小呢。就如我一开始说的那样我们要不停的更新里面的参数,而x就是我们要更新的那个参数。用数学表达式就是min(f(x+△x)），如果f(x+△x) &lt; f(x)恒成立，那就是说，我们的优化函数一直再减小，那么可以理解为一定会找到一个完美的结果，是不是想想就有点小开心呢，但是往往不会有那么好的事情的，就和人生一样。&emsp;&emsp;再接上面，那我们怎么样才能保证约等式的右边恒小于0呢，因为有：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;在这里，||·||表示向量的模，θ是向量▽f(x)和△x的夹角。因为向量的模一定大于等于0，所以说如果cosθ &lt;= 0,则能保证约等式的右边恒小于0。也就是选择合适的增量△x，就能保证函数数值下降，要达到这种效果，只要保证梯度和△x的夹角的余弦值小于等于0就行。由于cosθ &gt;= -1，当θ = π时，cosθ有极小值-1，此时梯度和△x反向，夹角为180度，因此当向量△X的模大小一定时，当：△X = - ▽f(x),也就是在梯度相反的方向函数值下降最快。此时有cosθ = -1。重点来了：那我们为什么要让△X = - ▽f(x)呢，让它等于别的不可以吗？这一块我考虑了好半天，最后我只能这么解释一下。如果我们想让约等式的右边小于等于0恒成立，怎么办！！怎么样才能保证最靠谱呢？那我不如让△X = - ▽f(x)算了，一个平方的值带负号是一定小于等于0的。这也太靠谱了吧！！我觉得这么取得意义就在这里，仅此而已。&emsp;&emsp;也就是说只要梯度不为0，往梯度的反方向走函数值一定是下降的。直接用△X = - ▽f(x)可能会有问题，因为x+△x可能会超出x的领域范围之外，此时是不能忽略泰勒展开中的二次及以上的项的，因此步伐不能太大，一般设△x = -α▽f(x)。其中α为一个接近于0的正数，称为步长，由人工设定。&emsp;&emsp;从初始点X_0开始，使用此迭代公式：X_k+1 = X_k - α▽f(x).&emsp;&emsp;只要没有到达梯度为0的点，则函数值会沿着序列X_k递减，最终会收敛到梯度为0的点，这就是梯度下降法。迭代终止的条件是函数的梯度值为0（实际是接近于0），此时认为已经达到了极值点。注意我们找到的是梯度为0的点，这不一定就是极值点。梯度下降法只需要计算函数在某些点处的梯度，实现简单，计算量小。&emsp;&emsp;再写一套比较直观的公式推导，看完有点原来如此，so easy的感觉~~&emsp;&emsp;$f(x_{k+1}) = f(x_k + x_{k+1} - x_k) = f(x_k) + ▽f(x_k)(x_{k+1} - x_k)$&emsp;&emsp;$▽f(x_k)$即为函数f在$x_k$处的梯度。&emsp;&emsp;为了使$f(x_{k+1}) &lt; f(x_k)$,则需要$▽f(x_k)(x_{k+1} - x_k)&lt;0$&emsp;&emsp;则只需要使$x_{k+1}=x_k - \\gamma▽f(x_k)$即可。&emsp;&emsp;上式便是梯度下降的迭代公式，其中$\\gamma$为学习速率。这样看是不是很清楚了呢。。。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"梯度下降","slug":"梯度下降","permalink":"https://pspxiaochen.club/tags/梯度下降/"}]},{"title":"从拉格朗日乘子法再到KKT条件的理解","slug":"lagrange and kkt","date":"2017-12-26T13:27:00.000Z","updated":"2017-12-26T13:31:10.714Z","comments":true,"path":"lagrange and kkt/","link":"","permalink":"https://pspxiaochen.club/lagrange and kkt/","excerpt":"","text":"&emsp;&emsp;西瓜书看了有一阵子了，终于看到了SVM，遥想研一看SVM的时候。。我操 这是什么。。这又是什么。。论高中数学的重要性的，出来混都是要还的。只好一点一点补齐SVM中用到的各种知识点。今天先扯一扯自己对拉格朗日乘子法和KKT条件的理解，希望这篇文章写完之后，能让我对这2个臭逼玩意能有更好的理解，那么现在开始。 我们为什么在SVM中要用到拉格朗日和KKT条件！！！&emsp;&emsp;因为在SVM中我们要计算一个有约束条件的极值问题，而拉格朗日乘子法和KKT条件是非常重要的两个求取方法。&emsp;&emsp;对于等式约束的优化问题（就是限制的那个式子是个等式 s.t h(x) = 0 类似这种），可以应用拉格朗日乘子法去求取最优值。&emsp;&emsp;如果含有不等式约束，可以应用KKT条件去求取最优值。&emsp;&emsp;这两种方法求得的结果只是必要条件，只有当是凸函数（二次导数大于0为凸）的情况下，才能保证是充分必要条件。而KKT条件是拉格朗日乘子法的泛化。&emsp;&emsp;下面我就针对这两个玩意，谈一谈我的理解。 拉格朗日乘子法！！！&emsp;&emsp;拉格朗日乘子法在考研的数学二中就用到过，当时只知道要那么用，那么用就能解决条件极值的问题，但是真心不知道为什么，这次先把为什么可以用拉格朗日说一下。&emsp;&emsp;想象一下，目标函数f(x,y)是一座山的高度，约束g(x,y)=C是镶嵌在山上的一条曲线如下图。（渣画技看看就好了）&emsp;&emsp;你为了找到曲线上的最低点，就从最低的等高线（0那条）开始网上数。数到第三条，等高线终于和曲线有交点了（如上图所示）。因为比这条等高线低的地方都不在约束范围内，所以这肯定是这条约束曲线的最低点了。&emsp;&emsp;而且约束曲线在这里不可能和等高线相交，一定是相切。因为如果是相交的话，如下图所示，那么曲线一定会有一部分在B区域，但是B区域比等高线低，这是不可能的。假设g(x)与等高线相交，交点就是同时满足等式约束条件和目标函数的可行域的值，但肯定不是最优值，因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小，只有到等高线与目标函数的曲线相切的时候，可能取得最优值。&emsp;&emsp;两条曲线相切，意味着他们在这点的法线平行，也就是法向量只差一个任意的常数乘子（取为-λ）：&emsp;&emsp;∇f（x, y) = -λ(∇g(x,y)-C) 把这个式子的右边移动到左边，就得到∇（f(x,y)+λ(g(x,y)-C)=0&emsp;&emsp; 在看看这个式子，你又能想起来什么呢？高中数学。。。这个就是f(x,y)+λ(g(x,y)-C)没有约束情况下极值点的充分条件吧。。（令其导数等于0，求极值点。。。高中用的就是这个套路。。哎）所以证明了拉格朗日乘子法确实可以解决这类问题，拉格朗日确实牛逼。。高中数学也应该好好学 哎。难受啊。&emsp;&emsp;那拉格朗日乘子法到底是怎么用的呢？简单的再说一下就是 把等式约束hi(x)用一个系数与f(x)写为一个式子，称为拉格朗日函数，而系数称为拉格朗日乘子。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。 KKT条件！！！！！&emsp;&emsp;我先给出KKT条件：&emsp;&emsp;对于具有等式和不等式约束的一般优化问题：&emsp;&emsp;&emsp;&emsp;KKT条件给出了判断X*是否是最优解的必要条件：&emsp;&emsp; 不等式约束优化问题&emsp;&emsp;我们先给出其主要思想：转化的思想——将不等式约束条件变成等式约束条件.具体做法：引入松弛变量.松弛变量也是优化变量，也需要一视同仁求偏导.&emsp;&emsp;具体来说，我们先看一个一元函数的例子：&emsp;&emsp;&emsp;&emsp;min f(x)&emsp;&emsp;s.t. g1(x) = a - x ≤ 0&emsp;&emsp;&emsp;&emsp;g2(x) = x - b ≤ 0&emsp;&emsp;（注：优化问题中，我们必须求得一个确定的值，因此不妨令所有的不等式均取到等号，即≤的情况.）&emsp;&emsp;对于约束g1和g2,我们分别引入两个松弛变量a₁²和b₁²,得到h1(x,a₁) = g1 + a₁² = 0和h2(x,b₁) = g2 + b₁² = 0,这里直接加上平方项a₁²和b₁²而非a₁和b₁,是因为g1和g2这两个不等式的左边必须加上一个正数（或者0）才能使不等式变成等式。若只加上a₁和b₁,又会引入新的约束a₁≥0,b₁≥0.这岂不是很爆炸！！！&emsp;&emsp;由此我们将不等式约束转化为了等式约束，并得到了拉格朗日函数L（x,a₁,b₁,u₁,u₂) = f(x) + u₁(a - x + a₁²) + u₂(x - b + b₁²)&emsp;&emsp;我们再按照等式约束优化问题对齐求解，联立方程组：（注：这里的u₁ ≥ 0,u₂ ≥ 0先承认，等会再解释，实际上对于不等式约束前的乘子，我们要求其大于等于0）得出方程组后，便开始手动解，先看第3行的两个式子 u₁a₁ = 0 和 u₂b₁ = 0比较简单，我们就从他们入手。对于u₁a₁ = 0,我们有两种情况:情形1: u₁ = 0,a₁ ≠ 0:此时由于乘子u₁ = 0,因此g1与其相乘为0，可以理解为约束g1不起作用，且有g1(x) = a - x &lt; 0 (因为a₁ ≠ 0 , 所以他的平方一定是大于0的）情形2：u₁ ≥ 0 , a₁ = 0此时，g1(x) = a - x = 0 且 u1 &gt; 0 (因为a₁ = 0 为了 a - x + a₁² = 0 ， 所以 a - x必须等于0），可以理解为约束g1起作用了，且有g1(x) = 0 合并两种情况的：u1g1 = 0 且在约束起作用时,u₁ &gt; 0,g1(x) = 0;约束不起作用时u1 = 0,g1(x) &lt; 0同样的，分析u₂b₁ = 0,可得出约束g2起作用和不起作用的情形，并分析得到u₂g₂ = 0由此，方程组（极值必要条件）转化为：这是一元一次的情形，类似的，对于多远多次不等式约束问题min f(x)s.t.g_j(x) ≤ 0 ( j = 1,2,````,m)我们有上式便称为不等式约束优化问题的KKT（Karush-Kuhn-Tucker）条件u_j称为KKT乘子，且约束起作用时u_j＞0,g_j(x) = 0;约束不起作用时u_j = 0,g_j(x) &lt; 0. 总结：同时包含等式和不等式约束的一般优化问题注意，对于等式约束的Lagrange乘子，并没有非负的要求！以后求其极值点，不必再引入松弛变量，直接使用KKT条件判断！","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://pspxiaochen.club/tags/SVM/"}]},{"title":"偏差与方差的总结","slug":"2017-12-12-bias-var","date":"2017-12-12T13:27:00.000Z","updated":"2017-12-26T03:11:52.843Z","comments":true,"path":"2017-12-12-bias-var/","link":"","permalink":"https://pspxiaochen.club/2017-12-12-bias-var/","excerpt":"","text":"&emsp;&emsp;今天开始重新仔细学习西瓜书（机器学习），今天看完了第一章，第一章基本都是一些概念的东西，公式也比较少，希望可以坚持把这本书看完。 基本概念&emsp;&emsp;根据各种公式推导得知，泛化误差 = 偏差 + 方差 +噪声&emsp;&emsp;而偏差（bias）度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。方差(Variance)度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。 偏差与方差的关系&emsp;&emsp;偏差与方差是有冲突的。给定学习任务，假定我们能控制学习算法的训练程度，则在训练不足时，模型的拟合能力不够强，训练数据的扰动不足以使模型产生显著变化，此时偏差主导了泛化错误率；随着训练程度的加深，模型的拟合能力逐渐增强，训练数据发生的扰动渐渐能被模型学习到，方差逐渐主导了泛化错误率；在训练程度充足后，模型的拟合能力已经非常强，训练数据发生的轻微扰动都会导致模型发生显著变化，若训练数据自身的，非全局性的特性被模型学习到了，则将发生过拟合。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://pspxiaochen.club/categories/机器学习/"}],"tags":[{"name":"偏差","slug":"偏差","permalink":"https://pspxiaochen.club/tags/偏差/"},{"name":"方差","slug":"方差","permalink":"https://pspxiaochen.club/tags/方差/"}]},{"title":"Sum of Two Integers（使用位运算实现）","slug":"2017-12-07-add","date":"2017-12-07T08:27:00.000Z","updated":"2017-12-07T08:37:43.607Z","comments":true,"path":"2017-12-07-add/","link":"","permalink":"https://pspxiaochen.club/2017-12-07-add/","excerpt":"","text":"思路&emsp;&emsp;两个数的加法分为两步，对应位相加和进位。&emsp;&emsp;举个简单的例子：997+24 &emsp;&emsp;我们平时计算时是将对应位相加和进位同时计算，其实可以保留下进位，只计算对应位相加，保留进位的位置（值）。接下来，将进位向左移动一位，将上一步的结果与移位后的进位值进行对应位相加，直到没有进位结束。 &emsp;&emsp;对于二进制数的而言，对应位相加就可以使用异或（xor）操作，计算进位就可以使用与（and）操作，在下一步进行对应位相加前，对进位数使用移位操作（&lt;&lt;）。 &emsp;&emsp;这样就非常好理解下面的实现代码。 12345678910int getSum(int a, int b)&#123; while (b) &#123; int c = a ^ b; b = (a &amp; b) &lt;&lt; 1; a = c; &#125; return a; &#125; &emsp;&emsp;最后，再给一个详细的运行过程示意，计算523+1125.（另外，如果是有负数的话，算法也是可行的，可以去看一下补码的相关内容）","raw":null,"content":null,"categories":[{"name":"刷题","slug":"刷题","permalink":"https://pspxiaochen.club/categories/刷题/"}],"tags":[{"name":"位运算","slug":"位运算","permalink":"https://pspxiaochen.club/tags/位运算/"},{"name":"异或","slug":"异或","permalink":"https://pspxiaochen.club/tags/异或/"}]},{"title":"峰度（Kurtosis）和偏度（Skewness）","slug":"2017-11-28-Kurtosis","date":"2017-11-28T11:12:00.000Z","updated":"2017-11-28T13:24:18.674Z","comments":true,"path":"2017-11-28-Kurtosis/","link":"","permalink":"https://pspxiaochen.club/2017-11-28-Kurtosis/","excerpt":"","text":"峰度（Kurtosis）&emsp;&emsp;峰度是描述总体中所有取值分布形态陡缓程度的统计量。这个统计量需要与正态分布相比较，峰度为0表示该总体数据分布与正态分布的陡缓程度相同；峰度大于0表示该总体数据分布与正态分布相比较为陡峭，为尖顶峰；峰度小于0表示该总体数据分布与正态分布相比较为平坦，为平顶峰。峰度的绝对值数值越大表示其分布形态的陡缓程度与正态分布的差异程度越大。&emsp;&emsp;峰度的具体计算公式为：&emsp;&emsp; 偏度（Skewness）&emsp;&emsp;偏度与峰度类似，它也是描述数据分布形态的统计量，其描述的是某总体取值分布的对称性。这个统计量同样需要与正态分布相比较，偏度为0表示其数据分布形态与正态分布的偏斜程度相同；偏度大于0表示其数据分布形态与正态分布相比为正偏或右偏，即有一条长尾巴拖在右边，数据右端有较多的极端值；偏度小于0表示其数据分布形态与正态分布相比为负偏或左偏，即有一条长尾拖在左边，数据左端有较多的极端值。偏度的绝对值数值越大表示其分布形态的偏斜程度越大。&emsp;&emsp;偏度的具体计算公式为：&emsp;&emsp;","raw":null,"content":null,"categories":[{"name":"其他","slug":"其他","permalink":"https://pspxiaochen.club/categories/其他/"}],"tags":[{"name":"Kurtosis","slug":"Kurtosis","permalink":"https://pspxiaochen.club/tags/Kurtosis/"}]},{"title":"Hexo博客撰写之：MarkDown语法介绍","slug":"2017-11-17-test","date":"2017-11-17T16:00:00.000Z","updated":"2017-11-20T08:03:16.896Z","comments":true,"path":"2017-11-17-test/","link":"","permalink":"https://pspxiaochen.club/2017-11-17-test/","excerpt":"","text":"前言搭建好了hexo，接下来的就是写博客了， hexo 支持用 markdown 写博客，markdown 语法很简单，本文给出一些基本的 markdown 语法，结合 hexo 教你如何在 hexo 下用 markdown 撰写博客。 hexo博客头部hexo 博客的 markdown 头部有固定的格式，如下所示： 1234567891011---title: Hexo博客撰写之：MarkDown语法介绍date: 2017-11-17 09:12:00categories: - 其他tags: - hexo - markdowndescription: 本文介绍如何在hexo搭建的博客下用markdown写文章,以及一些markdown的基本语法．photos: - https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike150%2C5%2C5%2C150%2C50/sign=62597ff6ba51f819e5280b18bbdd2188/908fa0ec08fa513d7ddc9503376d55fbb3fbd9fd.jpg 顾名思义，我们很容易看懂这个头部信息．我们写新博客的时候只需要复制这些内容，然后对相应的内容进行修改，但不要修改格式． hexo博客正文hexo 博客正文就是正常的 markdown 了，下面就介绍一些基本的 markdown 语法． 基本的markdown语法 标题通过在行首插入 1 到 6 个 # ，来定义 1 到 6 阶 标题： Markdown 预览 # 一级标题 #一级标题 ## 二级标题 ## 二级标题 ### 三级标题 ### 三级标题 段落和换行 在 Markdown 中段落由一行或者多行文本组成，相邻的两行文字会被视为同一段落，如果存在空行则被视为不同段落( Markdown 对空行的定义是看起来是空行就是空行，即使空行中存在 空格 TAB 回车 等不可见字符，同样会被视为空行)。 Markdown 预览 第一行相邻被视为统一段落 第一行 相邻被视为同一段落 第一行[空格][空格]上一行结尾存在两个空格，段内换行 第一行上一行结尾存在两个空格，段内换行 第一行两行之间存在空行，视为不同段落。 第一行两行之间存在空行，视为不同段落。 缩进 输入法中文全角状态下输入两个空格即可实现缩进．输入两个&amp;emsp;或&amp;ensp;也可以实现空格缩进． 强调 Markdown 预览 *倾斜* 倾斜 **粗体** 粗体 ~~删除线~~ 删除线 &gt;引用 &gt;引用 连接和图片 Markdown 预览 [百度一下](http://www.baidu.com) 百度一下 ![joker](https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1511008391023&amp;di=1ff315398f4671c899dc706aa34693de&amp;imgtype=0&amp;src=http%3A%2F%2Fimg3.duitang.com%2Fuploads%2Fitem%2F201203%2F17%2F20120317230833_kvzj3.thumb.700_0.jpeg) 音频和视频可以直接使用html的 和 标签嵌入 音频 和 视频．比如以下视频标签：Markdown:12345 &lt;center&gt; &lt;video width=320 height=200 src='http://tb-video.bdstatic.com/tieba-smallvideo/32_054ab95c5e7edd7a22c913fd1d1d8a5c.mp4' controls='controls' &gt; 您的浏览器不支持 video 标签。 &lt;/video&gt;&lt;/center&gt; 列表 无序列表用 - 引领列表内容， 有序列表用 数字 引领列表内容， 需要指出的是：有序列表的数字即便不按照顺序排列，结果仍是有序的。 下划线和特殊符号 由于 Markdown 使用一些特殊符号进行标记，当我们想要在文档中使用这些特殊符号并防止被 Markdown 转换的时候，可以使用 \\ (转义符) 将这些特殊符号进行转义。 Markdown 预览 在一行中用三个以上的减号来建立一个分隔线—- —- 可以利用反斜杠(转义字符)来插入一些在语法中有特殊意义的符号\\*Hi\\* *Hi* 代码 1 行内代码&emsp;行内代码可以使用反引号来标记(反引号一般位于键盘左上角，要用英文． Markdown 预览 一句话`行内代码`一句话 一句话行内代码 就比如` &lt; video &gt; `标签 就比如&lt; video &gt;标签 2 多行代码&emsp;多行代码使用 3 个反引号来标记(反引号一般位于键盘左上角，要用英文) ，在第一个 1234567Markdown:``` javascript ```javascript // 我是注释 var a = 5 ; console.log(a) 123456预览：```javascript// 我是注释var a = 5 ;console.log(a) 表格Markdown: | 默认 | 靠右 | 居中 | 靠左 || —— | —-: | :—: | :—- || 内容 | 内容 | 内容 | 内容 || 内容 | 内容 | 条目 | 内容 | 预览 默认 靠右 居中 靠左 内容 内容 内容 内容 内容 内容 条目 内容 OK先到这里,基本语法就是这样，以后如果学到新的好用语法了，我会继续推送的^_^","raw":null,"content":null,"categories":[{"name":"其他","slug":"其他","permalink":"https://pspxiaochen.club/categories/其他/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"https://pspxiaochen.club/tags/markdown/"},{"name":"hexo","slug":"hexo","permalink":"https://pspxiaochen.club/tags/hexo/"}]},{"title":"第一篇post：庆祝博客搭建成功","slug":"2017-11-20-first-post","date":"2017-11-16T16:00:00.000Z","updated":"2017-11-20T02:30:36.777Z","comments":true,"path":"2017-11-20-first-post/","link":"","permalink":"https://pspxiaochen.club/2017-11-20-first-post/","excerpt":"","text":"&emsp;&emsp;&ensp;我听中文音乐很少，先放一首比较喜欢的中文歌曲… &emsp; 《平凡之路》—-朴树 前言&emsp;&emsp;&ensp;一直想搭建自己的博客，但是一直没实践，只怪自己太懒。这次终于下定决心去实践，不过像我等小白来来回回折腾了好几天，本来想省点事直接在github上小改一下，谁知道老是出问题，一提交就收到Page build failure邮件，头疼的我差点放弃，最后狠下心来在本地安装了jekyll，一边调试一边修改，总算调通了，然后push到github，刷新主页，-_-，404找不到页面！妈呀，搜了好久解决办法，最后按照网上说的bundle update升级到了和github一样的最新版，这次报错了：MethedError：not find methed to_liquid for…想了半天，当前’github-pages’包的最新版是155,我把Gemfile中的’github-pages’改成了’github-pages’, ‘~&gt; 154’，也就是从默认的当前最新版本155降到了154,运行bundel exec jekyll serve，完美通过。。。我就郁闷了，原来这个jekyll-next主题已经不适用最新版的github-pages了。那怎么办呢，直接用hexo+next吧，索性静态页面就静态到底吧。 安装node和hexo（基于windows） 安装node去nodejs官网下载32或者64位的 node 安装包，然后在Windows下安装 node ，安装完成后，添加 node 到系统 PATH 变量，然后 Win+r 打开运行窗口，输入 cmd 打开命令窗口，然后键入： 1node -v 查看node是否已经安装好,再键入 1npm -v 安装Hexo这里先安装cnpm，以加快npm包的下载速度： 1npm install -g cnpm --registry=https://registry.npm.taobao.org 然后，安装hexo 1$ cnpm install hexo -g 打开cmd命令窗口,键入: 1hexo -v 查看hexo是否已安装好 安装git&emsp;&emsp;&ensp;去git for windows下载32或者64位的 git 安装包，然后在Windows下安装 git ，安装完成后，添加 git 到系统 PATH 变量，然后 Win+r 打开运行窗口，输入 cmd 打开命令窗口，然后键入：1git -v 查看git是否已经安装好 本地生成SSH key并添加到github 本地生成ssh keyhttps每次push需要输入用户名和密码，为了以后部署方便，我们使用ssh提交，使用ssh需要配置添加SSH key，具体如下：打开 git bash，输入以下命令: 12$ cd ~$ ssh-keygen -C \"your_computer_name\" 接着会提示输入文件名，默认就行了，Enter再接着会提示你输入两次密码，这个是push时候的密码，我们选择空密码，Enter没问题的话就成功了。 添加ssh key 到github 1$ clip &lt; ~/.ssh/id_rsa.pub 然后登录github，进入右上角Account Settings，然后点击菜单栏的SSH key进入页面添加key，点击Add SSH key按钮，把复制的SSH key代码粘贴到key所对应的输入框，点击确认，Title会默认使用你的”your_computer_name”。 测试该SSH key 1$ ssh -T git@github.com 出现 12$ Hi \" your-github-username \"! You've successfully authenticated, but GitHub does not provide shell access.$ Connection to github.com closed. ok,搞定。 搭建博客 新建github pages仓库注册github账号然后新建一个仓库，仓库名称为 your-github-username.github.io，比如我的是pspxiaochen.github.io 搭建博客在本地磁盘新建一个blog文件夹，比如在D盘新建一个blog文件夹，然后进入blog文件夹，执行以下操作：右键打开 git bash，输入以下命令 123$ git clone git@github.com:spaceJmmy/spaceJmmy-blog-template$ cd spaceJmmy-blog-template$ cnpm install 下载完成后,继续输入： 12$ hexo clean$ hexo s 如果出现 12INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop 说明启动成功，但是信息还是我的，所以接下来要修改配置博客了。 配置博客修改站点配置文件 spaceJmmy-blog-template/_config.yml： 修改站点信息，将以下内容改成你自己的信息： 12345# Sitetitle: spaceJmmy的博客 #博客名subtitle: 纯真容易幸福，单纯就易满足 #博客副标题description: #给搜索引擎看的，对站点的描述，可以自定义author: spaceJmmy #作者名称 修改站点 URL ，将站点 URL 改成你自己的 URL： 1234# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://pspxiaochen.github.ioroot: / 修改部署备份信息：把两个 git@github.com:spaceJmmy/spaceJmmy.github.io.git 换成成你自己的 repo 地址。 1234567891011# Deploymentdeploy: type: git repo: github: git@github.com:spaceJmmy/spaceJmmy.github.io.git,master message: updated at &#123;&#123; now(\"YYYY-MM-DD HH:mm:ss\") &#125;&#125; backup: type: git repository: github: git@github.com:spaceJmmy/spaceJmmy.github.io.git,src message: updated at &#123;&#123; now(\"YYYY-MM-DD HH:mm:ss\") &#125;&#125; 修改next主题配置文件 spaceJmmy-blog-template/themes/next/_config.yml： 修改 github 社交信息，将我的 GitHub 链接 https://github.com/spaceJmmy 改成你自己的链接：123social: #LinkLabel: Link GitHub: https://github.com/spaceJmmy 更换站点图标和用户头像： 更换站点图标更换本地文件夹 spaceJmmy-blog-template/themes/next/source 下面的 favicon.ico ，换成你自己的站点图标，文件名不要改变。 更换用户头像更换本地文件夹 spaceJmmy-blog-template/themes/next/source/images 下面的 avatar.gif ，换成你自己的用户头像，文件名不要改变。 修改关于页面： 修改文件夹 spaceJmmy-blog-template/source/about 下的 index.md 文件，改为你自己的 关于 页面。 测试配置是否成功 在git bash中输入以下命令： 12$ hexo clean $ hexo s 浏览器打开 http://localhost:4000/ ，如果成功的话，你会发现你的博客已经呈现出你的信息了，吼吼，狂欢吧…… 不过，先别急，先把网站部署备份了再说： OK，接下来部署备份你的网站，这时候在 bash 终端 Ctrl+C 停止服务器运行，然后输入： 1$ hexo d 你会发现静态网站已经 push 到你 repo 的 master 分支了。浏览器打开 your-github-username.github.io 就能看到你的博客了，哈哈…… 继续，备份博客源码之前需要先删除当前目录下的 .git 文件夹，然后 bash 输入： 1hexo b 你会发现网站源码已经备份到你 repo 的 src 分支了，至此，可以开心的庆祝啦，哈哈。 博客以后的常态化管理 以后写博客只需要自己写一个 .md 文件，然后放到/source/_posts文件夹下，写好博客后，来个拉风的部署三部曲，呼呼： 123$ hexo clean #清空缓存$ hexo d #部署站点到master分支$ hexo b #备份站点源代码到src分支 换台电脑重新部署（记得添加新的SSH key） 得益于前面的工作，换台电脑我们只需要clone仓库的src分支，然后重新生成hexo博客环境来撰写和发布post。 123$ git clone -b src git@github.com:your-github-username/your-github-username.github.io.git$ cd your-github-username.github.io$ cnpm install hexo环境搭建成功，然后 hexo s 本地预览，添加新的post，再按上述部署三部曲走起，呼呼… 有时 hexo b 会报错，提示执行 git push，那就 git push，你会看到 push 成功，哈哈。 &emsp;&emsp;&ensp;至此大功告成，看着自己现在这个博客上线，心里确实美滋滋啊，haha。 &emsp;&emsp;&ensp;这个博客的搭建，要感谢很多人… 首先感谢github，提供了git pages来托管我们的博客，而且是免费的; 然后要感谢提供主题模板的开源贡献者，使得像我这样的小白能够用上这么高大上的博客; 最后要感谢我自己，能够下定决心克服搭建博客的困难，谁让我是小白呢，慢慢进步。。。","raw":null,"content":null,"categories":[{"name":"其他","slug":"其他","permalink":"https://pspxiaochen.club/categories/其他/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://pspxiaochen.club/tags/hexo/"},{"name":"教程","slug":"教程","permalink":"https://pspxiaochen.club/tags/教程/"},{"name":"node","slug":"node","permalink":"https://pspxiaochen.club/tags/node/"},{"name":"git","slug":"git","permalink":"https://pspxiaochen.club/tags/git/"}]}]}